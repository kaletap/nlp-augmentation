{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "augmentation_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqp4A6lcASV6"
      },
      "source": [
        "# Augmentation pipeline: MLM Insertion\n",
        "This notebook serves as an example how to do build data processing and model training pipeline to run experiments on data augmentation. We consider MLM insertion augmentation as an example. The augmentation we use is simply augmentation on the fly when retrieving samples from torch's Dataset (like in Computer Vision augmentations). The disadvantage is clearly the speed of computation: model-based augmentation is computationally (and memory) expensive.\n",
        "\n",
        "On AGNews dataset of size 1000 it runs in 3.5 minutes compared to 2 without augmentation. I think it could be sped up when using more jobs for data loading (that would probably require using script and not notebook environment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWtJcEfMpygI",
        "outputId": "19f7fb8d-0a63-4d00-fd5f-a5b0f9e867d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "%pip install -U datasets\n",
        "%pip install transformers\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "ROOT_DIR = \"drive/My Drive/Colab Notebooks/nlp/results/ag_news_mlm_insertion\"\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    os.mkdir(ROOT_DIR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: datasets in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZW4JvsPxhff"
      },
      "source": [
        "## Defining augmentation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9a5MnCnxkNO"
      },
      "source": [
        "class MLMInsertionAugmenter:\n",
        "    def __init__(self, model, tokenizer, p: float, min_mask: int = 1, device=None):\n",
        "        self.model = model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_token = tokenizer.mask_token\n",
        "        self.mask_token_id = tokenizer.mask_token_id\n",
        "        self.min_mask = min_mask\n",
        "        self.p = p\n",
        "        self.device = device or torch.device('cpu')\n",
        "        \n",
        "    def __call__(self, text: str):\n",
        "        words = np.array(text.split(), dtype='>U6')\n",
        "        n_mask = max(self.min_mask, int(len(words) * self.p))\n",
        "        masked_indices = np.sort(np.random.choice(len(words) + 1, size=n_mask))\n",
        "\n",
        "        masked_words = np.insert(words, masked_indices, self.mask_token)\n",
        "        masked_text = \" \".join(masked_words)\n",
        "        \n",
        "        tokenizer_output = self.tokenizer([masked_text])\n",
        "        input_ids = torch.tensor(tokenizer_output['input_ids']).to(self.device)\n",
        "        attention_mask = torch.tensor(tokenizer_output['attention_mask']).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output = self.model(input_ids)\n",
        "\n",
        "        predicted_logits = output.logits[input_ids == self.mask_token_id]\n",
        "        predicted_tokens = predicted_logits.argmax(1)\n",
        "        predicted_words = [tokenizer.decode(token.item()).strip() for token in predicted_tokens]  # stripping to avoid multiple spaces\n",
        "        \n",
        "        new_words = np.insert(words, masked_indices, predicted_words)\n",
        "        new_text = \" \".join(new_words)\n",
        "        return new_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8s3gMywBdpA"
      },
      "source": [
        "class DatasetWithAugmentation(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, augmenter, augmentation_prob: float = 0.9):\n",
        "        self.dataset = dataset\n",
        "        self.augmenter = augmenter\n",
        "        self.augmentation_prob = augmentation_prob\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        item = self.dataset[i]\n",
        "        if random.random() < self.augmentation_prob:\n",
        "            item['text'] = self.augmenter(item['text'])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEFFI0Kdp4k6"
      },
      "source": [
        "def get_datasets(dataset_name, augmenter, train_size, val_size=5_000, test_size=None, augmentation_prob = 0.9, random_seed: int = 42):\n",
        "    \"\"\"Returns \"\"\"\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "    test_dataset = load_dataset(dataset_name, split=\"test\")\n",
        "    # We want test and validation data to be the same for every experiment\n",
        "    if test_size:\n",
        "        test_dataset = test_dataset.train_test_split(test_size=test_size, seed=random_seed)[\"test\"]\n",
        "    train_val_split = dataset.train_test_split(test_size=val_size, seed=random_seed)\n",
        "    # Validation and test sets\n",
        "    train_dataset = train_val_split[\"train\"].train_test_split(train_size=train_size, seed=random_seed)[\"train\"]\n",
        "    train_dataset = DatasetWithAugmentation(train_dataset, augmenter, augmentation_prob=augmentation_prob)\n",
        "    val_dataset = train_val_split[\"test\"]\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "class DataCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __call__(self, examples: List[dict]):\n",
        "        labels = [example['label'] for example in examples]\n",
        "        texts = [example['text'] for example in examples]\n",
        "        tokenizer_output = self.tokenizer(texts, truncation=True, padding=True)\n",
        "        return {\n",
        "            'labels': torch.tensor(labels), \n",
        "            'input_ids': torch.tensor(tokenizer_output['input_ids']), \n",
        "            'attention_mask': torch.tensor(tokenizer_output['attention_mask'])\n",
        "            }\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, micro_f1, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
        "    _, _, macro_f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'micro_f1': micro_f1,\n",
        "        'micro_precision': precision,\n",
        "        'micro_recall': recall,\n",
        "        'macro_f1': macro_f1\n",
        "    }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW2d5HGCqTTb",
        "outputId": "fd284306-5034-4a52-dc0f-b556119e77e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "AUGMENTATION_PROB = 0.5\n",
        "AUGMENTATION_FRACTION = 0.1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', return_dict=True, num_labels=4)\n",
        "data_collator = DataCollator(tokenizer)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', return_dict=True).eval().to(device)\n",
        "augmenter = MLMInsertionAugmenter(mlm_model, tokenizer, AUGMENTATION_FRACTION, device=device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld0FI0F7qhMm",
        "outputId": "e4f81440-3912-4a6b-d576-28e60d050f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "train_size = 1000\n",
        "train_dataset, val_dataset, test_dataset = get_datasets(\"ag_news\", augmenter, train_size, val_size=5_000, augmentation_prob=AUGMENTATION_PROB)\n",
        "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
        "print(train_dataset[0])\n",
        "print(val_dataset[0])\n",
        "print(test_dataset[0])\n",
        "output_dir = os.path.join(ROOT_DIR, f\"train_size_{train_size}\")\n",
        "\n",
        "num_train_epochs = 7\n",
        "\n",
        "# https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=0,  # don't have any intuition for the right value here\n",
        "    logging_dir=output_dir,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    remove_unused_columns=False,\n",
        "    no_cuda=False,\n",
        "    metric_for_best_model=\"eval_accuracy\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        "    \n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "test_result = trainer.evaluate(test_dataset)\n",
        "\n",
        "print(test_result)\n",
        "\n",
        "with open(os.path.join(output_dir, 'test_result.json'), 'w') as f:\n",
        "    json.dump(test_result, f, indent=4)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n",
            "Using custom data configuration default\n",
            "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-2db6642321dfdef3.arrow and /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-6ff55a4f79810bf1.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-96a0b3f12072cafc.arrow and /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-7f0413ec2ed47261.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 1000, Validation size: 5000, Test size: 7600\n",
            "{'label': 3, 'text': 'Study: High-t firms praise for online custom respec While many high-t firms scored well overal in a new study of the how they treat custom online more than a third of respon the survey compan still share person data withou permis'}\n",
            "{'label': 0, 'text': 'Bangladesh paralysed by strikes Opposition activists have brought many towns and cities in Bangladesh to a halt, the day after 18 people died in explosions at a political rally.'}\n",
            "{'label': 2, 'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='441' max='441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [441/441 11:36, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Micro F1</th>\n",
              "      <th>Micro Precision</th>\n",
              "      <th>Micro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.450826</td>\n",
              "      <td>0.329737</td>\n",
              "      <td>0.891800</td>\n",
              "      <td>0.891800</td>\n",
              "      <td>0.891800</td>\n",
              "      <td>0.891800</td>\n",
              "      <td>0.890798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.200697</td>\n",
              "      <td>0.349792</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.892341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.246281</td>\n",
              "      <td>0.385103</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.883295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.209226</td>\n",
              "      <td>0.377906</td>\n",
              "      <td>0.896800</td>\n",
              "      <td>0.896800</td>\n",
              "      <td>0.896800</td>\n",
              "      <td>0.896800</td>\n",
              "      <td>0.895536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.048192</td>\n",
              "      <td>0.441861</td>\n",
              "      <td>0.894600</td>\n",
              "      <td>0.894600</td>\n",
              "      <td>0.894600</td>\n",
              "      <td>0.894600</td>\n",
              "      <td>0.893212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.063677</td>\n",
              "      <td>0.488496</td>\n",
              "      <td>0.891200</td>\n",
              "      <td>0.891200</td>\n",
              "      <td>0.891200</td>\n",
              "      <td>0.891200</td>\n",
              "      <td>0.890379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.086549</td>\n",
              "      <td>0.479927</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.893098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='475' max='475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [475/475 00:43]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3638724684715271, 'eval_accuracy': 0.9040789473684211, 'eval_micro_f1': 0.9040789473684211, 'eval_micro_precision': 0.9040789473684211, 'eval_micro_recall': 0.9040789473684211, 'eval_macro_f1': 0.9037342459041019, 'epoch': 7.0, 'total_flos': 499592021664000}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}