{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqp4A6lcASV6"
      },
      "source": [
        "# Roberta Classifier on IMDB: baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWtJcEfMpygI",
        "outputId": "b4860a10-7817-41ab-e956-0e51ceebb17f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "%pip install datasets\n",
        "%pip install transformers\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "ROOT_DIR = \"drive/My Drive/Colab Notebooks/nlp/results/imdb_baseline\"\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    os.mkdir(ROOT_DIR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEFFI0Kdp4k6"
      },
      "source": [
        "def get_datasets(dataset_name, train_size, val_size=1_000, test_size=None, random_seed: int = 42):\n",
        "    \"\"\"Returns \"\"\"\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "    test_dataset = load_dataset(dataset_name, split=\"test\")\n",
        "    # We want test and validation data to be the same for every experiment\n",
        "    if test_size:\n",
        "        test_dataset = test_dataset.train_test_split(test_size=test_size, seed=random_seed)[\"test\"]\n",
        "    train_val_split = dataset.train_test_split(test_size=val_size, seed=random_seed)\n",
        "    # Validation and test sets\n",
        "    train_dataset = train_val_split[\"train\"].train_test_split(train_size=train_size, seed=random_seed)[\"train\"]\n",
        "    val_dataset = train_val_split[\"test\"]\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "class DataCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __call__(self, examples: List[dict]):\n",
        "        labels = [example['label'] for example in examples]\n",
        "        texts = [example['text'] for example in examples]\n",
        "        tokenizer_output = self.tokenizer(texts, truncation=True, padding=True)\n",
        "        return {\n",
        "            'labels': torch.tensor(labels), \n",
        "            'input_ids': torch.tensor(tokenizer_output['input_ids']), \n",
        "            'attention_mask': torch.tensor(tokenizer_output['attention_mask'])\n",
        "            }\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW2d5HGCqTTb"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
        "data_collator = DataCollator(tokenizer)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld0FI0F7qhMm",
        "outputId": "98915b0b-6b90-4de4-9175-249cb15de9f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TRAIN_SIZES = [20, 100, 1_000, 10_000]\n",
        "for train_size in TRAIN_SIZES:\n",
        "    train_dataset, val_dataset, test_dataset = get_datasets(\"imdb\", train_size, val_size=1_000, test_size=5_000)\n",
        "    print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
        "    print(train_dataset[0])\n",
        "    print(val_dataset[0])\n",
        "    print(test_dataset[0])\n",
        "    output_dir = os.path.join(ROOT_DIR, f\"train_size_{train_size}\")\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('roberta-base', return_dict=True)\n",
        "\n",
        "    # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
        "    training_args = TrainingArguments(\n",
        "        learning_rate=3e-5,\n",
        "        weight_decay=0.01,\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=6,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=2,  # actual batch size: 16 (as suggested in Bert paper)\n",
        "        warmup_steps=0,  # don't have any intuition for the right value here\n",
        "        logging_dir=output_dir,\n",
        "        logging_steps=5,\n",
        "        load_best_model_at_end=True,\n",
        "        evaluation_strategy='epoch',\n",
        "        remove_unused_columns=False,\n",
        "        no_cuda=False,\n",
        "        metric_for_best_model='eval_accuracy'\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "        \n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    test_result = trainer.evaluate(test_dataset)\n",
        "\n",
        "    print(test_result)\n",
        "\n",
        "    with open(os.path.join(output_dir, 'test_result.json'), 'w') as f:\n",
        "        json.dump(test_result, f, indent=4)\n",
        "\n",
        "    print()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3dfb39ac59303a4a.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-be0d19c5c8e15bf8.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3bc54fc53ade2e9b.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-7c318f8395866af8.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-732bbc5c6472cee6.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-a2827b06fdba1236.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 20, Validation size: 1000, Test size: 5000\n",
            "{'label': 0, 'text': 'After hearing the word of mouth of just how bad this film is I took the plunge and bought the DVD. That said everything previously mentioned about this film is true. For a film that claimed to have a budget in the millions it just does not show on the screen at all. The list of problems with the film could drag on forever. Chief amongst them is the film is simply too long. It dragged on for a few minutes short of 3 hours. Nearly an hour probably could have been cut off the run time had the editor simply removed the overabundance of scenes dealing with nothing more then the main character wandering around aimlessly. <br /><br />Secondly, as many had pointed out from the \"trailers\", the special effects are anything but special. The tripods looked OK in a few shots here and there but beyond that everything was grade-Z 1970\\'s or 1980\\'s quality. Probably the worst effects of all were the horses, which stiffly tottered back and forth as they moved. The heat ray effects were laughable, as people were reduced to bones that somehow were still able to flail about without any muscles. Also pitiful was the Thunderchild sequence, in which the Thunderchild, described in the book as an ironclad ram, looked nothing of the sort. Instead it resembled a World War 1 era destroyer, complete with deck guns (which fired but had no visible crew), and torpedo tubes. <br /><br />The colors and backgrounds were just as bad as the effects. Most laughable of all was a scene early on in which the main character and his wife go for a nighttime stroll and he points out Mars to her in the sky. Well, the sky is black, but the views of the characters and the landscape around them is broad daylight. There is also a very sharp demarcation between the real landscape, bathed in full sunlight, and the fake black night sky with overly large fuzzy stars. To detract even further, the color of the scenes made no sense. In some they are bathed in orange light. In others green light. In still others it\\'s blue light. In some instances the outsides are orange lit but the interiors of houses are green or blue. The frame-rate and camera is very shaky, giving everything a stuttering look.<br /><br />Finally, the acting is overall sub-par. One man portrays two characters who\\'s sole difference was one lacked a mustache. This led to some confusion at times as to who was who and where they all were. The English accents, even to American ears, are outrageous. <br /><br />In summary, this movie could very well make a claim to being the worst film released in recent times. I have not seen Gigli or some of the other recent flops but this one, because of it\\'s poor quality in every respect, must easily be worse then anything that mainstream Hollywood has put out. I would not be surprised if the movie makes it to the bottom 10 or 20 in the IMDb rankings. It\\'s a pity that Mystery Science Theater is not still around.'}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 04:26, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.695341</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.699740</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.701761</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.702497</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.944078</td>\n",
              "      <td>0.703700</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.944078</td>\n",
              "      <td>0.704191</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 02:42]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.6939023733139038, 'eval_accuracy': 0.5012, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'epoch': 5.666666666666667, 'total_flos': 39694639169520}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3dfb39ac59303a4a.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-be0d19c5c8e15bf8.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3bc54fc53ade2e9b.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-7c318f8395866af8.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-46ce5ff86477b919.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3bac033af5be1c4d.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "{'label': 0, 'text': \"A complete waste of time<br /><br />Halla Bol is a complete waste of time. The script and dialogues are poorly written, the direction is lacklustre and the acting borders on hammy.This movie was clearly aiming for the Rang De Basanti crowd but it falls far short of the mark because it does not have even one of the elements that made RDB connect with its audience_great script, terrific acting, good direction and a powerful social message that was never preached but shown.<br /><br />Compared to that near-masterpiece, Halla Bol takes a step backwards by resorting to scenes such as the hero taking a leak on the villain's Persian rug and the hero's mentor staring down bullets in a truck no less! All of this might have been acceptable in the 80s when there was a downturn in movie quality and bad movies like DivyaShakti and Phool Aur Kaante became big hits, but movie-making has become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 05:01, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.698697</td>\n",
              "      <td>0.689509</td>\n",
              "      <td>0.626000</td>\n",
              "      <td>0.598712</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.544922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.756999</td>\n",
              "      <td>0.691230</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.720537</td>\n",
              "      <td>0.642122</td>\n",
              "      <td>0.573000</td>\n",
              "      <td>0.303426</td>\n",
              "      <td>0.920792</td>\n",
              "      <td>0.181641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.657940</td>\n",
              "      <td>0.484581</td>\n",
              "      <td>0.807000</td>\n",
              "      <td>0.783389</td>\n",
              "      <td>0.920844</td>\n",
              "      <td>0.681641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.271657</td>\n",
              "      <td>0.343400</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.884802</td>\n",
              "      <td>0.877159</td>\n",
              "      <td>0.892578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.155584</td>\n",
              "      <td>0.326883</td>\n",
              "      <td>0.883000</td>\n",
              "      <td>0.885182</td>\n",
              "      <td>0.889546</td>\n",
              "      <td>0.880859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 02:42]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3153507113456726, 'eval_accuracy': 0.888, 'eval_f1': 0.8866396761133604, 'eval_precision': 0.8953393295175798, 'eval_recall': 0.8781074578989575, 'epoch': 5.923076923076923, 'total_flos': 208375167032400}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3dfb39ac59303a4a.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-be0d19c5c8e15bf8.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3bc54fc53ade2e9b.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-7c318f8395866af8.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 1000, Validation size: 1000, Test size: 5000\n",
            "{'label': 0, 'text': 'Some 25 year olds behave like teenagers, coping with the death of a high-school mate, trying to find their purpose in live and love. The script is so lame that I had to force myself to even finish this movie. Stay away from it. 1/10'}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [372/372 13:48, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.310725</td>\n",
              "      <td>0.375194</td>\n",
              "      <td>0.853000</td>\n",
              "      <td>0.839344</td>\n",
              "      <td>0.952854</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.343555</td>\n",
              "      <td>0.258118</td>\n",
              "      <td>0.922000</td>\n",
              "      <td>0.922311</td>\n",
              "      <td>0.941057</td>\n",
              "      <td>0.904297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.101529</td>\n",
              "      <td>0.349507</td>\n",
              "      <td>0.907000</td>\n",
              "      <td>0.914601</td>\n",
              "      <td>0.863085</td>\n",
              "      <td>0.972656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.117877</td>\n",
              "      <td>0.364106</td>\n",
              "      <td>0.923000</td>\n",
              "      <td>0.926316</td>\n",
              "      <td>0.908068</td>\n",
              "      <td>0.945312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.126492</td>\n",
              "      <td>0.405429</td>\n",
              "      <td>0.926000</td>\n",
              "      <td>0.928433</td>\n",
              "      <td>0.919540</td>\n",
              "      <td>0.937500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.417305</td>\n",
              "      <td>0.928000</td>\n",
              "      <td>0.930368</td>\n",
              "      <td>0.921456</td>\n",
              "      <td>0.939453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 02:42]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4196006953716278, 'eval_accuracy': 0.9246, 'eval_f1': 0.9259768309444336, 'eval_precision': 0.9072720277029627, 'eval_recall': 0.9454691259021651, 'epoch': 5.992, 'total_flos': 2125417729134240}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3dfb39ac59303a4a.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-be0d19c5c8e15bf8.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-3bc54fc53ade2e9b.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-7c318f8395866af8.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 10000, Validation size: 1000, Test size: 5000\n",
            "{'label': 0, 'text': \"I went to the movie as a Sneak Preview in Austria. So didn't have an idea what I am going to see. The story is very normal. The movie is very long , I believe it could have cut to 1/2 without causing any problems to the story. Its the type of movie you can see in a boring night which you want to get bored more ! Ashton Kutcher was very good . Kevin Costner is OK. The movie is speaking about the US Coast Guards, how they are trained , their life style and the problems they face. As there aren't much effects in the movie. So if you want to watch it , then no need to waste your money and time going to the Cinema. Would be more effective to watch it at home when it gets on DVDs.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3750/3750 1:42:41, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.157812</td>\n",
              "      <td>0.203382</td>\n",
              "      <td>0.932000</td>\n",
              "      <td>0.934489</td>\n",
              "      <td>0.922053</td>\n",
              "      <td>0.947266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.328357</td>\n",
              "      <td>0.240676</td>\n",
              "      <td>0.932000</td>\n",
              "      <td>0.935484</td>\n",
              "      <td>0.909594</td>\n",
              "      <td>0.962891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.063318</td>\n",
              "      <td>0.322237</td>\n",
              "      <td>0.933000</td>\n",
              "      <td>0.933990</td>\n",
              "      <td>0.942346</td>\n",
              "      <td>0.925781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.004620</td>\n",
              "      <td>0.365045</td>\n",
              "      <td>0.935000</td>\n",
              "      <td>0.935580</td>\n",
              "      <td>0.949698</td>\n",
              "      <td>0.921875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.093207</td>\n",
              "      <td>0.374098</td>\n",
              "      <td>0.939000</td>\n",
              "      <td>0.940604</td>\n",
              "      <td>0.937864</td>\n",
              "      <td>0.943359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.081525</td>\n",
              "      <td>0.398375</td>\n",
              "      <td>0.941000</td>\n",
              "      <td>0.942214</td>\n",
              "      <td>0.944990</td>\n",
              "      <td>0.939453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 02:42]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3703303635120392, 'eval_accuracy': 0.94, 'eval_f1': 0.9399279134961954, 'eval_precision': 0.9388, 'eval_recall': 0.9410585404971933, 'epoch': 6.0, 'total_flos': 21426663048011040}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9CA1BgFWfq4"
      },
      "source": [
        "Interestingly, model where training loss is close 0 (0.0008 compared to 0.1673) still has the same accuracy (0.940 instead of 0.938) as model without overfitting (even though validation loss is way worse: 0.398 compared to 0.213).\n",
        "\n"
      ]
    }
  ]
}