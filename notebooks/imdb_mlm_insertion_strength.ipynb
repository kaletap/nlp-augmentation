{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_mlm_insertion_strength.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqp4A6lcASV6"
      },
      "source": [
        "# IMDB: MLM Insertion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWtJcEfMpygI",
        "outputId": "d5341270-2433-4bfd-8e01-4958d9c4e3a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import heapq\n",
        "import os\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "%pip install -U datasets\n",
        "%pip install transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "ROOT_DIR = \"drive/My Drive/Colab Notebooks/nlp/results/imdb_mlm_insertion\"\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    os.mkdir(ROOT_DIR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/f4/2a3d6aee93ae7fce6c936dda2d7f534ad5f044a21238f85e28f0b205adf0/datasets-1.1.2-py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 199kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.2 pyarrow-2.0.0 xxhash-2.0.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 30.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f4f1f487f7e563be81c28e297c8df93dc5e9d891024750978d2fdead09ec6558\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZW4JvsPxhff"
      },
      "source": [
        "## Defining augmentation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9a5MnCnxkNO"
      },
      "source": [
        "class MLMInsertionAugmenter:\n",
        "    def __init__(self, model, tokenizer, p: float, min_mask: int = 1, max_mask: int = 100, topk: int = 5, uniform: bool = False, device=None):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.model = model.eval().to(self.device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_words = list(tokenizer.get_vocab().keys())\n",
        "        self.mask_token = tokenizer.mask_token\n",
        "        self.mask_token_id = tokenizer.mask_token_id\n",
        "        self.topk = topk\n",
        "        self.min_mask = min_mask\n",
        "        self.max_mask = max_mask\n",
        "        self.uniform = uniform\n",
        "        self.p = p\n",
        "        \n",
        "\n",
        "    def __call__(self, text: str):\n",
        "        if self.p == 0:\n",
        "            return text\n",
        "        words = np.array(text.split(), dtype='object')\n",
        "        n_mask = max(self.min_mask, int(len(words) * self.p))\n",
        "        n_mask = min(n_mask, self.max_mask)\n",
        "        max_masked_idx = min(self.tokenizer.model_max_length // 2 - n_mask, len(words) + 1)  # offset, since lenght might increase after tokenization\n",
        "        # end of the long text won't be augmented, but I guess we can live with that\n",
        "        masked_indices = np.sort(np.random.choice(max_masked_idx, size=n_mask, replace=False))\n",
        "        masked_words = np.insert(words, masked_indices, self.mask_token)\n",
        "        masked_text = \" \".join(masked_words)\n",
        "        \n",
        "        tokenizer_output = self.tokenizer([masked_text], truncation=True)\n",
        "        input_ids = torch.tensor(tokenizer_output['input_ids']).to(self.device)\n",
        "        attention_mask = torch.tensor(tokenizer_output['attention_mask']).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output = self.model(input_ids)\n",
        "            predicted_logits = output.logits[input_ids == self.mask_token_id]\n",
        "            predicted_probas = predicted_logits.softmax(1)\n",
        "            \n",
        "        predicted_words = [self.sample_word(probas).strip() for probas in predicted_probas]\n",
        "        \n",
        "        new_words = np.insert(words, masked_indices, predicted_words)\n",
        "        new_text = \" \".join(new_words)\n",
        "        return new_text\n",
        "    \n",
        "    \n",
        "    def sample_word(self, predicted_probas):\n",
        "        if hasattr(predicted_probas, 'tolist'):\n",
        "            predicted_probas = predicted_probas.tolist()\n",
        "        most_probable = heapq.nlargest(self.topk, zip(self.vocab_words, predicted_probas),  key=lambda t: t[1])\n",
        "        words, probas = zip(*most_probable)\n",
        "        word = random.choice(words) if self.uniform else random.choices(words, weights=probas)[0]\n",
        "        return self.tokenizer.convert_tokens_to_string(word).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEFFI0Kdp4k6"
      },
      "source": [
        "class DatasetWithAugmentation(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, augmenter, augmentation_prob: float = 0.9):\n",
        "        self.dataset = dataset\n",
        "        self.augmenter = augmenter\n",
        "        self.augmentation_prob = augmentation_prob\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        try:\n",
        "            item = self.dataset[i]\n",
        "            if random.random() < self.augmentation_prob:\n",
        "                item['text'] = self.augmenter(item['text'])\n",
        "            return item\n",
        "        except:\n",
        "            print(item)\n",
        "            raise Exception(f\"Something went wrong when augmenting item number {i}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "def get_datasets(dataset_name, augmenter, train_size, val_size=5_000, test_size=None, augmentation_prob=0.5, random_seed: int = 42):\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "    test_dataset = load_dataset(dataset_name, split=\"test\")\n",
        "    # We want test and validation data to be the same for every experiment\n",
        "    if test_size:\n",
        "        test_dataset = test_dataset.train_test_split(test_size=test_size, seed=random_seed)[\"test\"]\n",
        "    train_val_split = dataset.train_test_split(test_size=val_size, seed=random_seed)\n",
        "    # Validation and test sets\n",
        "    train_dataset = train_val_split[\"train\"].train_test_split(train_size=train_size, seed=random_seed)[\"train\"]\n",
        "    train_dataset = DatasetWithAugmentation(train_dataset, augmenter, augmentation_prob=augmentation_prob)\n",
        "    val_dataset = train_val_split[\"test\"]\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "class DataCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __call__(self, examples: List[dict]):\n",
        "        labels = [example['label'] for example in examples]\n",
        "        texts = [example['text'] for example in examples]\n",
        "        tokenizer_output = self.tokenizer(texts, truncation=True, padding=True)\n",
        "        return {\n",
        "            'labels': torch.tensor(labels), \n",
        "            'input_ids': torch.tensor(tokenizer_output['input_ids']), \n",
        "            'attention_mask': torch.tensor(tokenizer_output['attention_mask'])\n",
        "            }\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW2d5HGCqTTb",
        "outputId": "df845fa1-ee4c-4487-c74a-ad8f741448cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "AUGMENTATION_PROB = 0.6\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=False)  # we cannot use Fast tokenizer for MLMInsertionAugmenter\n",
        "data_collator = DataCollator(tokenizer)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', return_dict=True).eval().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwhkqPjlI35c"
      },
      "source": [
        "augmenter = MLMInsertionAugmenter(mlm_model, tokenizer, 0.2, min_mask=1, topk=10, uniform=False, device=torch.device('cuda'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANBvl4Hj4e4B",
        "outputId": "3e7480d7-9b8f-4c72-81f0-bed34447900c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "augmenter('I love you ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I really love you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld0FI0F7qhMm",
        "outputId": "99f43cfe-51f3-40fb-9e53-21d8169e0aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_size = 100\n",
        "\n",
        "\n",
        "FRACTIONS = [0, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6]\n",
        "accuracies = list()\n",
        "\n",
        "for fraction in FRACTIONS:\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('roberta-base', return_dict=True)\n",
        "    augmenter = MLMInsertionAugmenter(mlm_model, tokenizer, fraction, min_mask=1, topk=10, uniform=False, device=device)\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = get_datasets(\"imdb\", augmenter, train_size, val_size=1_000, test_size=5_000, augmentation_prob=AUGMENTATION_PROB)\n",
        "    print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
        "    print(f\"Augmentation fraction: {fraction}\")\n",
        "    print(train_dataset[0])\n",
        "    print(val_dataset[0])\n",
        "    print(test_dataset[0])\n",
        "    output_dir = os.path.join(ROOT_DIR, f\"train_size_{train_size}_augmentation_fraction_{fraction}\")\n",
        "\n",
        "    num_train_epochs = 8  # 6 is ideal for 100 samples, convergence point\n",
        "\n",
        "    # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
        "    training_args = TrainingArguments(\n",
        "        learning_rate=3e-5,\n",
        "        weight_decay=0.01,\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=0,  # don't have any intuition for the right value here\n",
        "        logging_dir=output_dir,\n",
        "        logging_steps=5,\n",
        "        load_best_model_at_end=False,\n",
        "        evaluation_strategy='epoch',\n",
        "        remove_unused_columns=False,\n",
        "        no_cuda=False,\n",
        "        metric_for_best_model=\"eval_accuracy\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "        \n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    test_result = trainer.evaluate(test_dataset)\n",
        "\n",
        "    print(test_result)\n",
        "    accuracies.append(test_result['eval_accuracy'])\n",
        "\n",
        "    with open(os.path.join(output_dir, 'test_result.json'), 'w') as f:\n",
        "        json.dump(test_result, f, indent=4)\n",
        "\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0\n",
            "{'label': 0, 'text': \"A complete waste of time<br /><br />Halla Bol is a complete waste of time. The script and dialogues are poorly written, the direction is lacklustre and the acting borders on hammy.This movie was clearly aiming for the Rang De Basanti crowd but it falls far short of the mark because it does not have even one of the elements that made RDB connect with its audience_great script, terrific acting, good direction and a powerful social message that was never preached but shown.<br /><br />Compared to that near-masterpiece, Halla Bol takes a step backwards by resorting to scenes such as the hero taking a leak on the villain's Persian rug and the hero's mentor staring down bullets in a truck no less! All of this might have been acceptable in the 80s when there was a downturn in movie quality and bad movies like DivyaShakti and Phool Aur Kaante became big hits, but movie-making has become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 02:55, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.716219</td>\n",
              "      <td>0.687837</td>\n",
              "      <td>0.492000</td>\n",
              "      <td>0.015504</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.748208</td>\n",
              "      <td>0.691555</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.739509</td>\n",
              "      <td>0.598468</td>\n",
              "      <td>0.605000</td>\n",
              "      <td>0.387597</td>\n",
              "      <td>0.939850</td>\n",
              "      <td>0.244141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.611929</td>\n",
              "      <td>0.462080</td>\n",
              "      <td>0.827000</td>\n",
              "      <td>0.811341</td>\n",
              "      <td>0.918519</td>\n",
              "      <td>0.726562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.305582</td>\n",
              "      <td>0.351154</td>\n",
              "      <td>0.883000</td>\n",
              "      <td>0.887824</td>\n",
              "      <td>0.871940</td>\n",
              "      <td>0.904297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.204962</td>\n",
              "      <td>0.348863</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.889113</td>\n",
              "      <td>0.918750</td>\n",
              "      <td>0.861328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.078436</td>\n",
              "      <td>0.364418</td>\n",
              "      <td>0.871000</td>\n",
              "      <td>0.883679</td>\n",
              "      <td>0.820771</td>\n",
              "      <td>0.957031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.041603</td>\n",
              "      <td>0.360705</td>\n",
              "      <td>0.883000</td>\n",
              "      <td>0.892955</td>\n",
              "      <td>0.839931</td>\n",
              "      <td>0.953125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3758549690246582, 'eval_accuracy': 0.8734, 'eval_f1': 0.8815715622076706, 'eval_precision': 0.8263767099263416, 'eval_recall': 0.9446672012830793, 'epoch': 7.923076923076923, 'total_flos': 275609850530400}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.01\n",
            "{'label': 0, 'text': \"A complete waste of time<br /><br - />Halla Bol is a complete waste of time. The script and dialogues are poorly written, the direction is lacklustre and the acting borders on hammy.This movie was clearly aiming for the Rang De Basanti crowd but it falls far short of the mark because it does not have even one of the elements that made RDB connect with its audience_great script, terrific acting, good direction and a powerful social message that was never just preached but shown.<br /><br />Compared to that near-masterpiece, Halla Bol takes a step backwards by resorting to scenes such as the hero taking a leak on the villain's Persian rug and the hero's mentor staring down bullets in a truck no less! All of this might have been acceptable in the 80s when there was a downturn in movie quality and bad movies like DivyaShakti and Phool Aur Kaante became big hits, but movie-making has become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence and in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 03:10, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.698254</td>\n",
              "      <td>0.691330</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.726529</td>\n",
              "      <td>0.703574</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.679125</td>\n",
              "      <td>0.587439</td>\n",
              "      <td>0.707000</td>\n",
              "      <td>0.649102</td>\n",
              "      <td>0.839009</td>\n",
              "      <td>0.529297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.651515</td>\n",
              "      <td>0.391155</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.870197</td>\n",
              "      <td>0.929047</td>\n",
              "      <td>0.818359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.246741</td>\n",
              "      <td>0.375480</td>\n",
              "      <td>0.866000</td>\n",
              "      <td>0.880357</td>\n",
              "      <td>0.810855</td>\n",
              "      <td>0.962891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.131075</td>\n",
              "      <td>0.318171</td>\n",
              "      <td>0.894000</td>\n",
              "      <td>0.895050</td>\n",
              "      <td>0.907631</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.067896</td>\n",
              "      <td>0.358716</td>\n",
              "      <td>0.887000</td>\n",
              "      <td>0.895853</td>\n",
              "      <td>0.848168</td>\n",
              "      <td>0.949219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.035709</td>\n",
              "      <td>0.399097</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.894204</td>\n",
              "      <td>0.845217</td>\n",
              "      <td>0.949219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.40014761686325073, 'eval_accuracy': 0.8806, 'eval_f1': 0.8881813073609289, 'eval_precision': 0.8333919156414763, 'eval_recall': 0.9506816359262229, 'epoch': 7.923076923076923, 'total_flos': 275932935995040}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.1\n",
            "{'label': 0, 'text': \"A complete waste of time<br /><br />Halla Bol is a complete waste of time. The script and dialogues are poorly written, the direction is lacklustre and the acting borders on hammy.This movie was clearly aiming for the Rang De Basanti crowd but it falls far short of the mark because it does not have even one of the elements that made RDB connect with its audience_great script, terrific acting, good direction and a powerful social message that was never preached but shown.<br /><br />Compared to that near-masterpiece, Halla Bol takes a step backwards by resorting to scenes such as the hero taking a leak on the villain's Persian rug and the hero's mentor staring down bullets in a truck no less! All of this might have been acceptable in the 80s when there was a downturn in movie quality and bad movies like DivyaShakti and Phool Aur Kaante became big hits, but movie-making has become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 04:31, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.705301</td>\n",
              "      <td>0.696263</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.741379</td>\n",
              "      <td>0.703639</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.725265</td>\n",
              "      <td>0.628018</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.299835</td>\n",
              "      <td>0.957895</td>\n",
              "      <td>0.177734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.663722</td>\n",
              "      <td>0.374893</td>\n",
              "      <td>0.889000</td>\n",
              "      <td>0.889552</td>\n",
              "      <td>0.906694</td>\n",
              "      <td>0.873047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.228510</td>\n",
              "      <td>0.312640</td>\n",
              "      <td>0.889000</td>\n",
              "      <td>0.890640</td>\n",
              "      <td>0.898608</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.151600</td>\n",
              "      <td>0.320074</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.889317</td>\n",
              "      <td>0.876660</td>\n",
              "      <td>0.902344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.065260</td>\n",
              "      <td>0.380225</td>\n",
              "      <td>0.884000</td>\n",
              "      <td>0.893382</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.949219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.027648</td>\n",
              "      <td>0.388671</td>\n",
              "      <td>0.883000</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>0.847100</td>\n",
              "      <td>0.941406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.36623984575271606, 'eval_accuracy': 0.8894, 'eval_f1': 0.8951261141665087, 'eval_precision': 0.8492263404102195, 'eval_recall': 0.946271050521251, 'epoch': 7.923076923076923, 'total_flos': 279531749087280}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.15\n",
            "{'label': 0, 'text': \": A complete waste of time<br /><br />Halla Bol is simply a complete waste of of time. The script and the dialogues are poorly written, the direction is extremely lacklustre and the acting borders on hammy.This movie was clearly aiming for pleasing the Rang De Basanti crowd but it falls far far short of the mark because it does , not possible have even half one of the elements that made The RDB movie connect with its audience_great script, terrific acting, a good direction and a powerful social message and that message was never preached , but shown.<br /><br />Compared to even that was near-masterpiece, Halla Bol takes a major step backwards by resorting to scenes such as depicting the hero accidentally taking out a gas leak on the villain's Persian rug and showing the dead hero's young mentor staring down bullets in a truck no less! All of this stuff might have once been acceptable back in the 80s when there really was a downturn in movie quality and bad movies like DivyaShakti and Hum Phool Aur Kaante became big hits, but movie-making has become_should probably have just become_more , subtle ty and less thoughtful of late.<br / /><br />Rajkumar Singh Santoshi is a very capable director , and indeed I really appreciate that he wants desperately to to give a social good message in every single movie he makes but , maybe he simply does not know exactly how to properly do it! :) He resorts mainly to sermonizing without taking a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 05:18, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.692519</td>\n",
              "      <td>0.689372</td>\n",
              "      <td>0.511000</td>\n",
              "      <td>0.089385</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.046875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.738916</td>\n",
              "      <td>0.698189</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.712880</td>\n",
              "      <td>0.620709</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.920245</td>\n",
              "      <td>0.292969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.643131</td>\n",
              "      <td>0.480913</td>\n",
              "      <td>0.817000</td>\n",
              "      <td>0.793687</td>\n",
              "      <td>0.938667</td>\n",
              "      <td>0.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.297647</td>\n",
              "      <td>0.328625</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.891635</td>\n",
              "      <td>0.868519</td>\n",
              "      <td>0.916016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.149837</td>\n",
              "      <td>0.351106</td>\n",
              "      <td>0.884000</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.883721</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.069949</td>\n",
              "      <td>0.422724</td>\n",
              "      <td>0.868000</td>\n",
              "      <td>0.878676</td>\n",
              "      <td>0.829861</td>\n",
              "      <td>0.933594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.023939</td>\n",
              "      <td>0.468808</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.876812</td>\n",
              "      <td>0.817568</td>\n",
              "      <td>0.945312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4674531817436218, 'eval_accuracy': 0.86, 'eval_f1': 0.871086556169429, 'eval_precision': 0.8055177111716622, 'eval_recall': 0.9482758620689655, 'epoch': 7.923076923076923, 'total_flos': 282101475144000}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.2\n",
            "{'label': 0, 'text': \"A complete waste of time<br / /><br />Halla De Bol 2 is also a complete bloody waste of time. The entire script lines and dialogues are all poorly well written, both the art direction itself is utterly lacklustre and the acting borders on the hammy.This entire movie was very clearly made aiming goal for the Rang De De Basanti crowd but it still falls far short short of the mark because it simply does not really have even one of the elements that made the RDB B connect easily with its target audience_great script, a terrific voice acting, good direction , and a powerful and social message that perhaps was never just preached but shown.<br /><br />Compared with to that original near-masterpiece, Halla Na Bol takes quite a step step backwards by resorting to scenes with such scenes as , the hero of taking out a water leak on the villain's Persian rug and the young hero's mentor , staring him down bullets in a truck cab no the less! . All of this might only have even been acceptable back in the 80s , when suddenly there was a downturn in in movie quality and bad movies , like the DivyaShakti and Phool Aur Kaante became big commercial hits, anything but movie-making has become_should have really become_more more subtle ty and thoughtful of late.<br /><br />Rajkumar Santoshi himself is a capable young director , and I appreciate that . he also wants to give a social justice message right in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 05:43, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.709064</td>\n",
              "      <td>0.691909</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.734651</td>\n",
              "      <td>0.693015</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.711291</td>\n",
              "      <td>0.592831</td>\n",
              "      <td>0.655000</td>\n",
              "      <td>0.516129</td>\n",
              "      <td>0.915423</td>\n",
              "      <td>0.359375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.629284</td>\n",
              "      <td>0.352925</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.888023</td>\n",
              "      <td>0.885437</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.223077</td>\n",
              "      <td>0.335087</td>\n",
              "      <td>0.878000</td>\n",
              "      <td>0.882012</td>\n",
              "      <td>0.873563</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.133890</td>\n",
              "      <td>0.333805</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.895238</td>\n",
              "      <td>0.873606</td>\n",
              "      <td>0.917969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.050578</td>\n",
              "      <td>0.384900</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.885636</td>\n",
              "      <td>0.833046</td>\n",
              "      <td>0.945312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.033046</td>\n",
              "      <td>0.372199</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.894539</td>\n",
              "      <td>0.863636</td>\n",
              "      <td>0.927734</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3598524034023285, 'eval_accuracy': 0.8866, 'eval_f1': 0.8904347826086957, 'eval_precision': 0.8593808280492353, 'eval_recall': 0.9238171611868484, 'epoch': 7.923076923076923, 'total_flos': 283348944021360}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.25\n",
            "{'label': 0, 'text': \"A complete waste of time<br /><br />Halla Bol is a complete waste of time. The script and dialogues are poorly written, the direction is lacklustre and the acting borders on hammy.This movie was clearly aiming for the Rang De Basanti crowd but it falls far short of the mark because it does not have even one of the elements that made RDB connect with its audience_great script, terrific acting, good direction and a powerful social message that was never preached but shown.<br /><br />Compared to that near-masterpiece, Halla Bol takes a step backwards by resorting to scenes such as the hero taking a leak on the villain's Persian rug and the hero's mentor staring down bullets in a truck no less! All of this might have been acceptable in the 80s when there was a downturn in movie quality and bad movies like DivyaShakti and Phool Aur Kaante became big hits, but movie-making has become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 06:28, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.694185</td>\n",
              "      <td>0.687053</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.683562</td>\n",
              "      <td>0.526371</td>\n",
              "      <td>0.974609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.760804</td>\n",
              "      <td>0.695415</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.735540</td>\n",
              "      <td>0.669448</td>\n",
              "      <td>0.571000</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>0.946237</td>\n",
              "      <td>0.171875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.703163</td>\n",
              "      <td>0.549109</td>\n",
              "      <td>0.774000</td>\n",
              "      <td>0.747768</td>\n",
              "      <td>0.872396</td>\n",
              "      <td>0.654297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.348930</td>\n",
              "      <td>0.342851</td>\n",
              "      <td>0.883000</td>\n",
              "      <td>0.882883</td>\n",
              "      <td>0.905544</td>\n",
              "      <td>0.861328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.188915</td>\n",
              "      <td>0.299791</td>\n",
              "      <td>0.894000</td>\n",
              "      <td>0.897683</td>\n",
              "      <td>0.887405</td>\n",
              "      <td>0.908203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.110460</td>\n",
              "      <td>0.356593</td>\n",
              "      <td>0.874000</td>\n",
              "      <td>0.884826</td>\n",
              "      <td>0.831615</td>\n",
              "      <td>0.945312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.073904</td>\n",
              "      <td>0.336830</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.896030</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.925781</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3488401174545288, 'eval_accuracy': 0.8838, 'eval_f1': 0.888505085396277, 'eval_precision': 0.8520426941479573, 'eval_recall': 0.9282277465918204, 'epoch': 7.923076923076923, 'total_flos': 285754135813680}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.3\n",
            "{'label': 0, 'text': \"A complete waste of time<br /><br />Halla Bol is a complete waste of time. The script and dialogues are poorly written, the direction is lacklustre and the acting borders on hammy.This movie was clearly aiming for the Rang De Basanti crowd but it falls far short of the mark because it does not have even one of the elements that made RDB connect with its audience_great script, terrific acting, good direction and a powerful social message that was never preached but shown.<br /><br />Compared to that near-masterpiece, Halla Bol takes a step backwards by resorting to scenes such as the hero taking a leak on the villain's Persian rug and the hero's mentor staring down bullets in a truck no less! All of this might have been acceptable in the 80s when there was a downturn in movie quality and bad movies like DivyaShakti and Phool Aur Kaante became big hits, but movie-making has become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 07:21, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.695967</td>\n",
              "      <td>0.688265</td>\n",
              "      <td>0.569000</td>\n",
              "      <td>0.527930</td>\n",
              "      <td>0.600998</td>\n",
              "      <td>0.470703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.735515</td>\n",
              "      <td>0.693479</td>\n",
              "      <td>0.489000</td>\n",
              "      <td>0.003899</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.725423</td>\n",
              "      <td>0.640239</td>\n",
              "      <td>0.637000</td>\n",
              "      <td>0.502058</td>\n",
              "      <td>0.843318</td>\n",
              "      <td>0.357422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.681234</td>\n",
              "      <td>0.463978</td>\n",
              "      <td>0.835000</td>\n",
              "      <td>0.823906</td>\n",
              "      <td>0.908235</td>\n",
              "      <td>0.753906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.321624</td>\n",
              "      <td>0.366354</td>\n",
              "      <td>0.856000</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.804636</td>\n",
              "      <td>0.949219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.199695</td>\n",
              "      <td>0.340217</td>\n",
              "      <td>0.878000</td>\n",
              "      <td>0.874227</td>\n",
              "      <td>0.925764</td>\n",
              "      <td>0.828125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.136621</td>\n",
              "      <td>0.315313</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.892221</td>\n",
              "      <td>0.857658</td>\n",
              "      <td>0.929688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.109499</td>\n",
              "      <td>0.316525</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.891612</td>\n",
              "      <td>0.861566</td>\n",
              "      <td>0.923828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3144931495189667, 'eval_accuracy': 0.8848, 'eval_f1': 0.8885880077369439, 'eval_precision': 0.8583707025411061, 'eval_recall': 0.9210104250200482, 'epoch': 7.923076923076923, 'total_flos': 288267022760880}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.4\n",
            "{'label': 0, 'text': \"| A near complete utter waste out of its time<br / /><br />Halla De Bol , is just a near complete colossal waste waste of its time. There The lackluster script and the dialogues alone are all poorly written, the camera direction itself is lacklustre , and the supporting acting style borders en on hammy.This was movie that was clearly aiming out for for the mainstream Rang 's De Re Basanti crowd , but actually it falls so far short of the mark is because all it really does is not even have achieved even one one of the core elements that it made the RDB movie connect with with its audience_great written script, a terrific character acting, good direction , and also a powerful , social good message , that it was simply never being preached about but shown.<br /><br br />Compared to even that near-masterpiece, here Halla Bol takes is a step backwards by only resorting merely to unnecessary scenes such as as when the hero taking out a gas leak on to the villain's red Persian rug , and the hero's old mentor simply staring him down with bullets in a a truck , no no less! There All of of this stuff might not have even been an acceptable back in during the late 80s when even there too was quite a severe downturn in the movie quality and really bad movies like DivyaShakti and Om Phool te Aur Aur Kaante became big money hits, and but movie-making philosophy has become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 07:55, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.695338</td>\n",
              "      <td>0.689194</td>\n",
              "      <td>0.518000</td>\n",
              "      <td>0.117216</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.062500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.731576</td>\n",
              "      <td>0.708201</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.736680</td>\n",
              "      <td>0.570711</td>\n",
              "      <td>0.828000</td>\n",
              "      <td>0.824131</td>\n",
              "      <td>0.864807</td>\n",
              "      <td>0.787109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.605769</td>\n",
              "      <td>0.331316</td>\n",
              "      <td>0.882000</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.881783</td>\n",
              "      <td>0.888672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.155972</td>\n",
              "      <td>0.318298</td>\n",
              "      <td>0.892000</td>\n",
              "      <td>0.895551</td>\n",
              "      <td>0.886973</td>\n",
              "      <td>0.904297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.103222</td>\n",
              "      <td>0.365906</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.899341</td>\n",
              "      <td>0.867514</td>\n",
              "      <td>0.933594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.037498</td>\n",
              "      <td>0.384570</td>\n",
              "      <td>0.894000</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.893411</td>\n",
              "      <td>0.900391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.065141</td>\n",
              "      <td>0.433754</td>\n",
              "      <td>0.884000</td>\n",
              "      <td>0.884462</td>\n",
              "      <td>0.902439</td>\n",
              "      <td>0.867188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4064241349697113, 'eval_accuracy': 0.8912, 'eval_f1': 0.8893409275834012, 'eval_precision': 0.902559867877787, 'eval_recall': 0.8765036086607859, 'epoch': 7.923076923076923, 'total_flos': 290256391594080}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.5\n",
            "{'label': 0, 'text': \"A near complete waste of your time<br / /><br / />Halla De Bol , is simply a a complete and waste out of my time. There The entire script and dialogues , are poorly written, and the film direction too is lacklustre , and the acting quality borders on on hammy.This one movie that was clearly the aiming goal for reaching the Rang 's De Da Basanti crowd , but unfortunately it still falls far far short short of hitting the right mark because when it , does not really have a even a one hundred of the core elements that that made RDB B connect well with its intended audience_great written script, a terrific acting, good art direction and even a very powerful and social message , that message was never preached upon but shown.<br / /><br / />Compared just to its that near-masterpiece, here Halla Bol takes us a huge step backwards by resorting to boring scenes with such moments as the hero taking out a gas leak , on to the evil villain's new Persian bed rug , and the young hero's mentor , staring down bullets rolling in from a truck no the less! :( All all of this might not have actually been so acceptable in the the mid 80s , when unfortunately there was a serious downturn ing in movie quality and other bad looking movies like DivyaShakti Ka and Phool Ka Aur Hai Kaante which became the big international hits, but but our movie-making culture has now become_should have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 08:21, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.714047</td>\n",
              "      <td>0.693319</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.741185</td>\n",
              "      <td>0.707702</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.712967</td>\n",
              "      <td>0.546726</td>\n",
              "      <td>0.766000</td>\n",
              "      <td>0.719424</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.585938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.583515</td>\n",
              "      <td>0.313443</td>\n",
              "      <td>0.889000</td>\n",
              "      <td>0.892546</td>\n",
              "      <td>0.884837</td>\n",
              "      <td>0.900391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.145199</td>\n",
              "      <td>0.311798</td>\n",
              "      <td>0.882000</td>\n",
              "      <td>0.887619</td>\n",
              "      <td>0.866171</td>\n",
              "      <td>0.910156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.106202</td>\n",
              "      <td>0.396843</td>\n",
              "      <td>0.884000</td>\n",
              "      <td>0.880412</td>\n",
              "      <td>0.932314</td>\n",
              "      <td>0.833984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.120908</td>\n",
              "      <td>0.381148</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.891683</td>\n",
              "      <td>0.883142</td>\n",
              "      <td>0.900391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.063165</td>\n",
              "      <td>0.391337</td>\n",
              "      <td>0.887000</td>\n",
              "      <td>0.889971</td>\n",
              "      <td>0.887379</td>\n",
              "      <td>0.892578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3604607880115509, 'eval_accuracy': 0.8942, 'eval_f1': 0.8942634419348391, 'eval_precision': 0.8915902750099641, 'eval_recall': 0.896952686447474, 'epoch': 7.923076923076923, 'total_flos': 290017069027680}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-79de4e42d72fb27e.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5ee71dc8a1d591db.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-d700ab48d9c5a8df.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-ee4811c8670e792a.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2d711dac5d060894.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-615a6f73083b52f6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 100, Validation size: 1000, Test size: 5000\n",
            "Augmentation fraction: 0.6\n",
            "{'label': 0, 'text': \": A complete waste of time<br /><br / />Halla Ko Bol 2 is a very complete , waste out of time. The terrible script , and the dialogues , are poorly well written, the acting direction is extremely lacklustre quality and frankly the poor acting borders in on the hammy.This is movie which was clearly an aiming point for the Rang 's De Basanti review crowd but it ultimately falls so far far short of of hitting the mark simply because what it does , not even have been even one one one of all the very elements which that made that RDB connect well with all its intended audience_great script, terrific acting, good direction and a very powerful , social message that it was to never just preached upon but rather shown.<br /><br / />Compared to that near-masterpiece, the Halla Ka Bol takes is a big step backwards , by often resorting only to stupid scenes , such as the hero accidentally taking out a leak based on with the evil villain's cheap Persian rug , and even the young hero's mentor , staring down the bullets coming in to a truck no no less! All of this this above might not have even been an acceptable back in late the 80s when , there 's was such a massive downturn in the movie 's quality and some bad movies like Dum DivyaShakti Hai and Dum Phool Aur Kaante that became relatively big budget hits, nothing but his movie-making culture has also become_should not have become_more subtle and thoughtful of late.<br /><br />Rajkumar Santoshi is a capable director and I appreciate that he wants to give a social message in every movie he makes but maybe he simply does not know how to do it! He resorts to sermonizing without a care as to the audience's intelligence in understanding what he is trying to say. Maybe he should just concentrate on entertainment and leave the social messages to the Rakeysh Mehras and Aamir Khans.<br /><br />Even if you don't agree with everything I say, you will agree that throughout the screening you will be thinking that Rang De Basanti was much much better and Mr.Santoshi should have left the industry-bashing to Om Shanti Om. Industry-bashing? That's right!!Santoshi has depicted the industry as a place of back-biting, bitching and the casting couch which the hero happily indulges in with a starlet curiously named Sania. There are some people who will think that these portions show the real face of the industry. Don't believe everything you see!<br /><br />All in all, raise your voice against movies like this and don't spend your hard-earned money on this bomb.<br /><br />* out of ****.\"}\n",
            "{'label': 0, 'text': 'This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'}\n",
            "{'label': 0, 'text': 'CONGO is probably the worst big-budget movie of the 1990s. It is so bad that it is watchable over and over again. A bunch of folks with different agendas probe deep into darkest Africa, where they encounter a temple of riches just like in KING SOLOMON\\'S MINES -- but with cannibalistic gorillas guarding those riches. On their side, the humans have a \"talking\" gorilla named Amy who helps save the day at one point or another. So much for the plot. The dialogue throughout is witless, the acting almost uniformly atrocious, certainly campy (Tim Curry and Ernie Hudson are both on hand to assure the ham factor), and the special effects abysmal. Wait until you see the cannibalistic gorillas. And the lava! Oh yes, did I mention there\\'s a volcanic eruption to top off the big finale? Laura Linney of all people is along for this bumpiest of rides. What could she have been thinking? This is right around the same time she appeared with Richard Gere and Ed Norton in PRIMAL FEAR, a movie that helped guarantee her stardom.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 09:06, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.712571</td>\n",
              "      <td>0.689451</td>\n",
              "      <td>0.559000</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.865979</td>\n",
              "      <td>0.164062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.743604</td>\n",
              "      <td>0.700167</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.730907</td>\n",
              "      <td>0.643200</td>\n",
              "      <td>0.559000</td>\n",
              "      <td>0.246154</td>\n",
              "      <td>0.986301</td>\n",
              "      <td>0.140625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.638660</td>\n",
              "      <td>0.404929</td>\n",
              "      <td>0.871000</td>\n",
              "      <td>0.873405</td>\n",
              "      <td>0.877712</td>\n",
              "      <td>0.869141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.215413</td>\n",
              "      <td>0.355479</td>\n",
              "      <td>0.872000</td>\n",
              "      <td>0.871227</td>\n",
              "      <td>0.898340</td>\n",
              "      <td>0.845703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.177916</td>\n",
              "      <td>0.471723</td>\n",
              "      <td>0.842000</td>\n",
              "      <td>0.861646</td>\n",
              "      <td>0.780952</td>\n",
              "      <td>0.960938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.194659</td>\n",
              "      <td>0.417498</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.876138</td>\n",
              "      <td>0.820819</td>\n",
              "      <td>0.939453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.421125</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.884793</td>\n",
              "      <td>0.837696</td>\n",
              "      <td>0.937500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 01:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.39792555570602417, 'eval_accuracy': 0.873, 'eval_f1': 0.8798031421540792, 'eval_precision': 0.8332735747579778, 'eval_recall': 0.9318364073777065, 'epoch': 7.923076923076923, 'total_flos': 288799515471120}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9q58htPR1pm",
        "outputId": "81c76435-5d03-403c-d882-61e26fd661ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "accuracies = [0.8734, 0.8806, 0.8894, 0.86, 0.8866, 0.8838, 0.8848, 0.8912, 0.8942, 0.873]\n",
        "FRACTIONS = [0, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6]\n",
        "plt.plot(FRACTIONS, accuracies)\n",
        "plt.scatter(FRACTIONS, accuracies)\n",
        "plt.title('IMDB')\n",
        "plt.xlabel('Augmentation strength')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig(os.path.join(ROOT_DIR, 'imdb_1000_mlm_insertion_strength.eps'), format='eps', bbox_inches='tight')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}