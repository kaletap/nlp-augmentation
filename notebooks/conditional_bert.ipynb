{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conditional_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2ZiEkON8l4"
      },
      "source": [
        "## Conditional Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iW93jsMxnj3",
        "outputId": "3f8eed63-c0fd-4cd0-e6e3-a4c9e7b381da"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "import nltk\n",
        "\n",
        "REPO_LOCATION = 'https://gitlab.com/korzeniowski.renard/text-augmentation.git'\n",
        "REPO_NAME = 'text-augmentation'\n",
        "REPO_BRANCH = 'master'\n",
        "PACKAGES = [\"datasets\", \"fastai\", \"nlpaug\", \"transformers\"]\n",
        "\n",
        "# Clone the repository\"\n",
        "if os.path.exists(REPO_NAME):\n",
        "    print(\"Removing existing repo\")\n",
        "    shutil.rmtree(REPO_NAME)\n",
        "print('cloning the repository')\n",
        "subprocess.call(['git', 'clone', '-b', REPO_BRANCH, REPO_LOCATION])\n",
        "\n",
        "# Setting env variables\n",
        "sys.path.append(REPO_NAME)\n",
        "\n",
        "# Install packages \n",
        "print('installing packages:', ', '.join(PACKAGES))\n",
        "subprocess.call(['pip', 'install -U'] + PACKAGES)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Run code\n",
        "os.chdir(REPO_NAME)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cloning the repository\n",
            "installing packages: datasets, fastai, nlpaug, transformers\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpX1F6oIYq1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32c31a9-2a32-4499-bfb9-6f2a578f483d"
      },
      "source": [
        "%pip install datasets\n",
        "%pip install -U fastai\n",
        "%pip install nlpaug\n",
        "%pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 206kB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.3 pyarrow-2.0.0 xxhash-2.0.0\n",
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/53/da994550c0dd2962351fd694694e553afe0c9516c02251586790f830430b/fastai-2.1.8-py3-none-any.whl (189kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Collecting fastcore>=1.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/0c/a08940a410b3c55b082578b7142e96afa1c8e3f39972b99366093bb4af59/fastcore-1.3.10-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: torchvision>=0.8 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.6/dist-packages (from fastai) (19.3.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.0.4)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.0.4)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastai) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->fastai) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->fastai) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai) (3.4.0)\n",
            "Installing collected packages: fastcore, fastai\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-2.1.8 fastcore-1.3.10\n",
            "Collecting nlpaug\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/40/18941536abc63578010e87808089eb3184a8d027df03bfc226894698f491/nlpaug-1.1.0-py3-none-any.whl (380kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 13.8MB/s \n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 24.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=eb4f80e5228cbc928e243bd5330176309b1adf157104076fec7b73d52f09e3d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm7UYbUudqa2"
      },
      "source": [
        "# Training Conditional Masked Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9VljmPwUUGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d14fa0-b30d-49fa-ae98-7951eef3280c"
      },
      "source": [
        "!python3 run_conditional_language_modeling.py \\\n",
        "--output_dir /content/drive/MyDrive/Colab\\ Notebooks/nlp/pretrained_models/yelp_conditional \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--dataset_name yelp \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--gradient_accumulation_steps 4 \\\n",
        "--num_train_epochs 10 \\\n",
        "--logging_steps 1 \\\n",
        "--save_steps 10 \\\n",
        "--eval_steps 10 \\\n",
        "--evaluation_strategy steps \\\n",
        "--num_train_epochs 10 \\\n",
        "--overwrite_output_dir"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-07 21:12:07.890000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/07/2020 21:12:10 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/07/2020 21:12:10 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec07_21-12-10_4892d22f056d', logging_first_step=False, logging_steps=1, save_steps=10, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "[INFO|configuration_utils.py:411] 2020-12-07 21:12:10,941 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:447] 2020-12-07 21:12:10,942 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:411] 2020-12-07 21:12:10,962 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:447] 2020-12-07 21:12:10,962 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1768] 2020-12-07 21:12:11,044 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1768] 2020-12-07 21:12:11,045 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1768] 2020-12-07 21:12:11,045 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|modeling_utils.py:940] 2020-12-07 21:12:11,133 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1056] 2020-12-07 21:12:15,479 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[WARNING|modeling_utils.py:1059] 2020-12-07 21:12:15,480 >> Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Reusing dataset yelp_polarity (/root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n",
            "Reusing dataset yelp_polarity (/root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-fc3cc76f500726b8.arrow and /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-24ad08dc80bfc802.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-726ca3418d6d9d46.arrow and /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-4f519a9136cf60de.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-c7c2ccaaab3d1b04.arrow and /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-799019c0c125c4f8.arrow\n",
            "Example from conditional dataset: {'label': 0, 'text': \"negative </s> Let me just start by saying DO NOT BOOK THIS HOTEL... I will give you our experience is short then explain everything in full. Within the first 10 hours we were switched to 3 different rooms, treated like crap by all the employees and rooms smell either like cat piss or dirty trash!!! Okay here it goes:\\\\nSo this is everything off the top of my head... mix in extreme attitude from the staff as if they hate their jobs and are being forced to work there\\\\n1. We had to walk through the whole casino floor with 9 bags to the check in at 9:45pm\\\\n2. Check in line was 45 minutes which we understand is not their fault they were busy but at least be nice and smile \\\\n3. We go to the far tower with 9 bags to our room and our key doesnt work\\\\n4. We wait 25 minutes in a hot hallway for security to open our door you then acts like it isnt our room as if we are stupid and went to the wrong door and knocks over and over on the door\\\\n5. We have to add our other night online because they couldn't do it at the front desk check in, you can only book a room for that day at the front desks\\\\n6. call guest services to see where we go for a new key, she talks to us like we are a stupid child \\\\n7. We walk back down to the main lobby to  get new keys and to have them mark that we are staying in the same room, they said thats fine \\\\n8. We go over to talk to someone about all the problems and she tells us we cant stay in the same room because someone booked it for tomorrow, so she gives us a new room\\\\n9. We go get our bags and take them to the next room, same tower but up stairs... room smells funny and looks nothing like the pictures and has an old school tv in it\\\\n10. The light in our new bathroom is broken, we say f##k it we havent ate all day so lets go eat first\\\\n11. Its after midnight so everything is closed except a un appealing Deli, Krispy Cream and Vince Neils restaurant so we eat there \\\\n12. Back at the room we call to have the light fixed 2 hours later they still haven't come to fix it  so we cancel so we can go to bed\\\\n13. We wake up and call to have the light fixed. She hangs up on me after I tell her what's going on\\\\n14. We call to speak to a manager, they hang up on us \\\\n15. We call back and demand to speak to a manager, he finally gets on the phone we explain everything he says he is moving our room and dropping the charge for the night\\\\n16. I call bell service while my friend goes down and talks to the manager and get new keys, they tell me to call back because they arent going to come up get the bags and bring them down and then go back up (well what the heck  that's what we have been doing since we get there!!!) \\\\n17. Maid service just walks in no knock nothing and then is bitchy because we are still in the room and she is going to have to come back\\\\n18. We get the new room number, bell man comes up gets  The bags and takes us in a service elevator to our new room which is better then the last but smells like a dirty trash in a woman's restroom (if you know what I mean... dirty sanitary stuff) and again looks nothing like the pictures online\\\\n19. We have to go down stairs because it looks like he charged the upgrade card. He says it is a refund but its in the wrong column of the receipt, they're charge column is actually credits and the credits is actually charges... if this is true why don't they just fix that in the system?\\\\n20. We left a item in our room and called to tell them, they said it hadn't been turned in and to try back. In the next 24 hours before we left we called 3 times and each time they said no it hasn't been turned in or reported. So basically we know it was left on the bed and now our childs doll that we know was left in the bed magically got up and walked away within 2 hours from the first time we called!!! \\\\n\\\\nThe employees are so rude and just when you think something has been fixed for you something else goes wrong and they give you an attitude about it and talk down to you as if some how you being in the hotel is a bother. CUSTOMER SERVICE TRAINING IS NEEDED BY ALL STAFF!!! \\\\n\\\\nMind you my husband (who was not with us) works at a casino in San Diego and he was appalled by everything we were telling him, he said their hotel would be ashamed and be doing everything they could to fix everything and everything would have been comp'd (food everything) not just one night. I wasnt asking for all that I would have been happy with an apology, fixing the issue right away, no rudeness or something. I mean they treat you like they could careless if you come back and like you are bothering them by being there. Mind you we were very passive and not angry or rude at all like we should have been, and like the other people who were down at the front desks complaining!!! I don't like leaving bad reviews but this one is much needed!!!\"}\n",
            "100% 1/1 [00:00<00:00,  2.42ba/s]\n",
            "100% 1/1 [00:00<00:00, 46.36ba/s]\n",
            "100% 1/1 [00:02<00:00,  2.44s/ba]\n",
            "100% 1/1 [00:00<00:00, 72.58ba/s]\n",
            "[INFO|trainer.py:357] 2020-12-07 21:12:23,683 >> The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:357] 2020-12-07 21:12:23,683 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:662] 2020-12-07 21:12:23,688 >> ***** Running training *****\n",
            "[INFO|trainer.py:663] 2020-12-07 21:12:23,688 >>   Num examples = 332\n",
            "[INFO|trainer.py:664] 2020-12-07 21:12:23,688 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:665] 2020-12-07 21:12:23,688 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:666] 2020-12-07 21:12:23,688 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:667] 2020-12-07 21:12:23,688 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:668] 2020-12-07 21:12:23,689 >>   Total optimization steps = 100\n",
            "{'loss': 1.709947943687439, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.09523809523809523}\n",
            "{'loss': 1.726820707321167, 'learning_rate': 4.9e-05, 'epoch': 0.19047619047619047}\n",
            "{'loss': 1.7759863138198853, 'learning_rate': 4.85e-05, 'epoch': 0.2857142857142857}\n",
            "{'loss': 1.7898359298706055, 'learning_rate': 4.8e-05, 'epoch': 0.38095238095238093}\n",
            "{'loss': 1.769895315170288, 'learning_rate': 4.75e-05, 'epoch': 0.47619047619047616}\n",
            "{'loss': 1.8532311916351318, 'learning_rate': 4.7e-05, 'epoch': 0.5714285714285714}\n",
            "{'loss': 1.688284158706665, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.6666666666666666}\n",
            "{'loss': 1.7395397424697876, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.7619047619047619}\n",
            "{'loss': 1.7077503204345703, 'learning_rate': 4.55e-05, 'epoch': 0.8571428571428571}\n",
            "{'loss': 1.7262909412384033, 'learning_rate': 4.5e-05, 'epoch': 0.9523809523809523}\n",
            " 10% 10/100 [00:50<07:54,  5.27s/it][INFO|trainer.py:1333] 2020-12-07 21:13:14,677 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:13:14,677 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:13:14,677 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.25it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.6255594491958618, 'epoch': 0.9523809523809523}\n",
            " 10% 10/100 [00:52<07:54,  5.27s/it]\n",
            "100% 3/3 [00:01<00:00,  3.19it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:13:15,832 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-10\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:13:15,840 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-10/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:13:18,322 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-10/pytorch_model.bin\n",
            "{'loss': 2.6225709915161133, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.0952380952380953}\n",
            "{'loss': 1.721538782119751, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.1904761904761905}\n",
            "{'loss': 1.6619999408721924, 'learning_rate': 4.35e-05, 'epoch': 1.2857142857142856}\n",
            "{'loss': 1.6031396389007568, 'learning_rate': 4.3e-05, 'epoch': 1.380952380952381}\n",
            "{'loss': 1.7735936641693115, 'learning_rate': 4.25e-05, 'epoch': 1.4761904761904763}\n",
            "{'loss': 1.7290312051773071, 'learning_rate': 4.2e-05, 'epoch': 1.5714285714285714}\n",
            "{'loss': 1.6998932361602783, 'learning_rate': 4.15e-05, 'epoch': 1.6666666666666665}\n",
            "{'loss': 1.7168326377868652, 'learning_rate': 4.1e-05, 'epoch': 1.7619047619047619}\n",
            "{'loss': 1.848813772201538, 'learning_rate': 4.05e-05, 'epoch': 1.8571428571428572}\n",
            "{'loss': 1.6558403968811035, 'learning_rate': 4e-05, 'epoch': 1.9523809523809523}\n",
            " 20% 20/100 [01:52<06:58,  5.24s/it][INFO|trainer.py:1333] 2020-12-07 21:14:15,914 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:14:15,915 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:14:15,915 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.41it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.6207584142684937, 'epoch': 1.9523809523809523}\n",
            " 20% 20/100 [01:53<06:58,  5.24s/it]\n",
            "100% 3/3 [00:01<00:00,  3.27it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:14:17,045 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-20\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:14:17,053 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-20/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:14:19,516 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-20/pytorch_model.bin\n",
            "{'loss': 2.5258772373199463, 'learning_rate': 3.9500000000000005e-05, 'epoch': 2.0952380952380953}\n",
            "{'loss': 1.6543829441070557, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.1904761904761907}\n",
            "{'loss': 1.5217247009277344, 'learning_rate': 3.85e-05, 'epoch': 2.2857142857142856}\n",
            "{'loss': 1.6704061031341553, 'learning_rate': 3.8e-05, 'epoch': 2.380952380952381}\n",
            "{'loss': 1.6889716386795044, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.4761904761904763}\n",
            "{'loss': 1.7199971675872803, 'learning_rate': 3.7e-05, 'epoch': 2.571428571428571}\n",
            "{'loss': 1.6077215671539307, 'learning_rate': 3.65e-05, 'epoch': 2.6666666666666665}\n",
            "{'loss': 1.6866354942321777, 'learning_rate': 3.6e-05, 'epoch': 2.761904761904762}\n",
            "{'loss': 1.5821261405944824, 'learning_rate': 3.55e-05, 'epoch': 2.857142857142857}\n",
            "{'loss': 1.5687756538391113, 'learning_rate': 3.5e-05, 'epoch': 2.9523809523809526}\n",
            " 30% 30/100 [02:54<06:06,  5.24s/it][INFO|trainer.py:1333] 2020-12-07 21:15:18,347 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:15:18,347 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:15:18,347 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.43it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.5790996551513672, 'epoch': 2.9523809523809526}\n",
            " 30% 30/100 [02:55<06:06,  5.24s/it]\n",
            "100% 3/3 [00:01<00:00,  3.32it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:15:19,458 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-30\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:15:19,466 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-30/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:15:22,453 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-30/pytorch_model.bin\n",
            "{'loss': 2.468350410461426, 'learning_rate': 3.45e-05, 'epoch': 3.0952380952380953}\n",
            "{'loss': 1.656278371810913, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.1904761904761907}\n",
            "{'loss': 1.5790326595306396, 'learning_rate': 3.35e-05, 'epoch': 3.2857142857142856}\n",
            "{'loss': 1.699749231338501, 'learning_rate': 3.3e-05, 'epoch': 3.380952380952381}\n",
            "{'loss': 1.6502327919006348, 'learning_rate': 3.2500000000000004e-05, 'epoch': 3.4761904761904763}\n",
            "{'loss': 1.4905345439910889, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.571428571428571}\n",
            "{'loss': 1.563918948173523, 'learning_rate': 3.15e-05, 'epoch': 3.6666666666666665}\n",
            "{'loss': 1.5344555377960205, 'learning_rate': 3.1e-05, 'epoch': 3.761904761904762}\n",
            "{'loss': 1.6092783212661743, 'learning_rate': 3.05e-05, 'epoch': 3.857142857142857}\n",
            "{'loss': 1.703736662864685, 'learning_rate': 3e-05, 'epoch': 3.9523809523809526}\n",
            " 40% 40/100 [03:57<05:18,  5.31s/it][INFO|trainer.py:1333] 2020-12-07 21:16:21,177 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:16:21,177 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:16:21,177 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.37it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.3979401588439941, 'epoch': 3.9523809523809526}\n",
            " 40% 40/100 [03:58<05:18,  5.31s/it]\n",
            "100% 3/3 [00:01<00:00,  3.28it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:16:22,299 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-40\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:16:22,306 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-40/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:16:24,795 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-40/pytorch_model.bin\n",
            "{'loss': 2.532332420349121, 'learning_rate': 2.95e-05, 'epoch': 4.095238095238095}\n",
            "{'loss': 1.5471516847610474, 'learning_rate': 2.9e-05, 'epoch': 4.190476190476191}\n",
            "{'loss': 1.6206308603286743, 'learning_rate': 2.8499999999999998e-05, 'epoch': 4.285714285714286}\n",
            "{'loss': 1.614525556564331, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.380952380952381}\n",
            "{'loss': 1.5523574352264404, 'learning_rate': 2.7500000000000004e-05, 'epoch': 4.476190476190476}\n",
            "{'loss': 1.6438891887664795, 'learning_rate': 2.7000000000000002e-05, 'epoch': 4.571428571428571}\n",
            "{'loss': 1.5879836082458496, 'learning_rate': 2.6500000000000004e-05, 'epoch': 4.666666666666667}\n",
            "{'loss': 1.5548090934753418, 'learning_rate': 2.6000000000000002e-05, 'epoch': 4.761904761904762}\n",
            "{'loss': 1.6112138032913208, 'learning_rate': 2.5500000000000003e-05, 'epoch': 4.857142857142857}\n",
            "{'loss': 1.606536865234375, 'learning_rate': 2.5e-05, 'epoch': 4.9523809523809526}\n",
            " 50% 50/100 [05:03<04:29,  5.38s/it][INFO|trainer.py:1333] 2020-12-07 21:17:27,061 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:17:27,061 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:17:27,061 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.36it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.5175174474716187, 'epoch': 4.9523809523809526}\n",
            " 50% 50/100 [05:04<04:29,  5.38s/it]\n",
            "100% 3/3 [00:01<00:00,  3.26it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:17:28,189 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-50\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:17:28,195 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-50/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:17:30,673 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-50/pytorch_model.bin\n",
            "{'loss': 2.3199520111083984, 'learning_rate': 2.45e-05, 'epoch': 5.095238095238095}\n",
            "{'loss': 1.5014300346374512, 'learning_rate': 2.4e-05, 'epoch': 5.190476190476191}\n",
            "{'loss': 1.540936827659607, 'learning_rate': 2.35e-05, 'epoch': 5.285714285714286}\n",
            "{'loss': 1.545424222946167, 'learning_rate': 2.3000000000000003e-05, 'epoch': 5.380952380952381}\n",
            "{'loss': 1.6354830265045166, 'learning_rate': 2.25e-05, 'epoch': 5.476190476190476}\n",
            "{'loss': 1.5832157135009766, 'learning_rate': 2.2000000000000003e-05, 'epoch': 5.571428571428571}\n",
            "{'loss': 1.631306529045105, 'learning_rate': 2.15e-05, 'epoch': 5.666666666666667}\n",
            "{'loss': 1.477159023284912, 'learning_rate': 2.1e-05, 'epoch': 5.761904761904762}\n",
            "{'loss': 1.5542070865631104, 'learning_rate': 2.05e-05, 'epoch': 5.857142857142857}\n",
            "{'loss': 1.5627379417419434, 'learning_rate': 2e-05, 'epoch': 5.9523809523809526}\n",
            " 60% 60/100 [06:05<03:30,  5.27s/it][INFO|trainer.py:1333] 2020-12-07 21:18:28,789 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:18:28,789 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:18:28,789 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.40it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.5245198011398315, 'epoch': 5.9523809523809526}\n",
            " 60% 60/100 [06:06<03:30,  5.27s/it]\n",
            "100% 3/3 [00:01<00:00,  3.28it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:18:29,913 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-60\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:18:29,922 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-60/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:18:32,498 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-60/pytorch_model.bin\n",
            "{'loss': 2.2690091133117676, 'learning_rate': 1.9500000000000003e-05, 'epoch': 6.095238095238095}\n",
            "{'loss': 1.6537327766418457, 'learning_rate': 1.9e-05, 'epoch': 6.190476190476191}\n",
            "{'loss': 1.6047050952911377, 'learning_rate': 1.85e-05, 'epoch': 6.285714285714286}\n",
            "{'loss': 1.5552912950515747, 'learning_rate': 1.8e-05, 'epoch': 6.380952380952381}\n",
            "{'loss': 1.6437304019927979, 'learning_rate': 1.75e-05, 'epoch': 6.476190476190476}\n",
            "{'loss': 1.482191562652588, 'learning_rate': 1.7000000000000003e-05, 'epoch': 6.571428571428571}\n",
            "{'loss': 1.4788779020309448, 'learning_rate': 1.65e-05, 'epoch': 6.666666666666667}\n",
            "{'loss': 1.5962803363800049, 'learning_rate': 1.6000000000000003e-05, 'epoch': 6.761904761904762}\n",
            "{'loss': 1.4687718152999878, 'learning_rate': 1.55e-05, 'epoch': 6.857142857142857}\n",
            "{'loss': 1.5124969482421875, 'learning_rate': 1.5e-05, 'epoch': 6.9523809523809526}\n",
            " 70% 70/100 [07:07<02:38,  5.29s/it][INFO|trainer.py:1333] 2020-12-07 21:19:31,562 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:19:31,562 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:19:31,562 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.50it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.4826487302780151, 'epoch': 6.9523809523809526}\n",
            " 70% 70/100 [07:08<02:38,  5.29s/it]\n",
            "100% 3/3 [00:01<00:00,  3.32it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:19:32,687 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-70\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:19:32,694 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-70/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:19:35,262 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-70/pytorch_model.bin\n",
            "{'loss': 2.217729091644287, 'learning_rate': 1.45e-05, 'epoch': 7.095238095238095}\n",
            "{'loss': 1.4921330213546753, 'learning_rate': 1.4000000000000001e-05, 'epoch': 7.190476190476191}\n",
            "{'loss': 1.5653676986694336, 'learning_rate': 1.3500000000000001e-05, 'epoch': 7.285714285714286}\n",
            "{'loss': 1.5531220436096191, 'learning_rate': 1.3000000000000001e-05, 'epoch': 7.380952380952381}\n",
            "{'loss': 1.5799665451049805, 'learning_rate': 1.25e-05, 'epoch': 7.476190476190476}\n",
            "{'loss': 1.5719252824783325, 'learning_rate': 1.2e-05, 'epoch': 7.571428571428571}\n",
            "{'loss': 1.4414455890655518, 'learning_rate': 1.1500000000000002e-05, 'epoch': 7.666666666666667}\n",
            "{'loss': 1.546478033065796, 'learning_rate': 1.1000000000000001e-05, 'epoch': 7.761904761904762}\n",
            "{'loss': 1.5642768144607544, 'learning_rate': 1.05e-05, 'epoch': 7.857142857142857}\n",
            "{'loss': 1.6933035850524902, 'learning_rate': 1e-05, 'epoch': 7.9523809523809526}\n",
            " 80% 80/100 [08:11<01:46,  5.32s/it][INFO|trainer.py:1333] 2020-12-07 21:20:35,444 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:20:35,444 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:20:35,444 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.40it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.5767160654067993, 'epoch': 7.9523809523809526}\n",
            " 80% 80/100 [08:12<01:46,  5.32s/it]\n",
            "100% 3/3 [00:01<00:00,  3.28it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:20:36,569 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-80\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:20:36,576 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-80/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:20:39,457 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-80/pytorch_model.bin\n",
            "{'loss': 2.247209072113037, 'learning_rate': 9.5e-06, 'epoch': 8.095238095238095}\n",
            "{'loss': 1.54207444190979, 'learning_rate': 9e-06, 'epoch': 8.19047619047619}\n",
            "{'loss': 1.641657829284668, 'learning_rate': 8.500000000000002e-06, 'epoch': 8.285714285714286}\n",
            "{'loss': 1.487906575202942, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.380952380952381}\n",
            "{'loss': 1.5249826908111572, 'learning_rate': 7.5e-06, 'epoch': 8.476190476190476}\n",
            "{'loss': 1.4997951984405518, 'learning_rate': 7.000000000000001e-06, 'epoch': 8.571428571428571}\n",
            "{'loss': 1.5526642799377441, 'learning_rate': 6.5000000000000004e-06, 'epoch': 8.666666666666666}\n",
            "{'loss': 1.4685900211334229, 'learning_rate': 6e-06, 'epoch': 8.761904761904763}\n",
            "{'loss': 1.5281367301940918, 'learning_rate': 5.500000000000001e-06, 'epoch': 8.857142857142858}\n",
            "{'loss': 1.4330430030822754, 'learning_rate': 5e-06, 'epoch': 8.952380952380953}\n",
            " 90% 90/100 [09:14<00:53,  5.30s/it][INFO|trainer.py:1333] 2020-12-07 21:21:38,592 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:21:38,592 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:21:38,592 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.41it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.497554898262024, 'epoch': 8.952380952380953}\n",
            " 90% 90/100 [09:16<00:53,  5.30s/it]\n",
            "100% 3/3 [00:01<00:00,  3.29it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:21:39,712 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-90\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:21:39,720 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-90/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:21:42,225 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-90/pytorch_model.bin\n",
            "{'loss': 2.2720611095428467, 'learning_rate': 4.5e-06, 'epoch': 9.095238095238095}\n",
            "{'loss': 1.5394096374511719, 'learning_rate': 4.000000000000001e-06, 'epoch': 9.19047619047619}\n",
            "{'loss': 1.4034425020217896, 'learning_rate': 3.5000000000000004e-06, 'epoch': 9.285714285714286}\n",
            "{'loss': 1.5803192853927612, 'learning_rate': 3e-06, 'epoch': 9.380952380952381}\n",
            "{'loss': 1.580775499343872, 'learning_rate': 2.5e-06, 'epoch': 9.476190476190476}\n",
            "{'loss': 1.4543228149414062, 'learning_rate': 2.0000000000000003e-06, 'epoch': 9.571428571428571}\n",
            "{'loss': 1.5084856748580933, 'learning_rate': 1.5e-06, 'epoch': 9.666666666666666}\n",
            "{'loss': 1.5612332820892334, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.761904761904763}\n",
            "{'loss': 1.4570016860961914, 'learning_rate': 5.000000000000001e-07, 'epoch': 9.857142857142858}\n",
            "{'loss': 1.538017988204956, 'learning_rate': 0.0, 'epoch': 9.952380952380953}\n",
            "100% 100/100 [10:20<00:00,  5.37s/it][INFO|trainer.py:1333] 2020-12-07 21:22:43,958 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:22:43,958 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:22:43,958 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.40it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.5595982074737549, 'epoch': 9.952380952380953}\n",
            "100% 100/100 [10:21<00:00,  5.37s/it]\n",
            "100% 3/3 [00:01<00:00,  3.27it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 21:22:45,084 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-100\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:22:45,091 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-100/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:22:47,651 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-100/pytorch_model.bin\n",
            "[INFO|trainer.py:801] 2020-12-07 21:22:53,211 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 9.952380952380953}\n",
            "100% 100/100 [10:29<00:00,  6.30s/it]\n",
            "[INFO|trainer.py:1162] 2020-12-07 21:22:53,243 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 21:22:53,254 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 21:23:04,902 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/pytorch_model.bin\n",
            "12/07/2020 21:23:05 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1333] 2020-12-07 21:23:05,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 21:23:05,040 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 21:23:05,040 >>   Batch size = 8\n",
            "100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "12/07/2020 21:23:06 - INFO - __main__ -   ***** Eval results *****\n",
            "12/07/2020 21:23:06 - INFO - __main__ -     perplexity = 4.4901463456736925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bl9LhsDINHO"
      },
      "source": [
        "# Evaluating Standard MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVUfZ2dZIZtb"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFLSK0XfKEmU",
        "outputId": "f0959154-429d-4d3d-9671-2170078e61a4"
      },
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained('roberta-base').eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "vocab_words = [tokenizer.decode(i) for i in range(len(tokenizer))]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x4REZ-P4IfnU",
        "outputId": "07bace1b-c33d-44de-9e50-679c885d57bc"
      },
      "source": [
        "sentence = f'I think this restaurant is the {tokenizer.mask_token} in the city'\n",
        "sentence"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I think this restaurant is the <mask> in the city'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgSQHFfxIpwX"
      },
      "source": [
        "tokenizer_output = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = tokenizer_output['input_ids']\n",
        "with torch.no_grad():\n",
        "    logits = model(**tokenizer_output).logits"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZFOUOwYKi5G"
      },
      "source": [
        "masked_position = (input_ids == tokenizer.mask_token_id).long().argmax().item()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sXfduu6JFvj",
        "outputId": "c3ffe5e6-b57a-4137-9072-2ff4d67944ac"
      },
      "source": [
        "probas = logits[0, masked_position, :].detach().softmax(0).numpy()\n",
        "words_probas = sorted(zip(vocab_words, probas), key=lambda t: t[1], reverse=True)\n",
        "words_probas[:10]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' best', 0.94857913),\n",
              " (' finest', 0.0108271325),\n",
              " (' coolest', 0.005482896),\n",
              " (' worst', 0.0053674923),\n",
              " (' newest', 0.004469985),\n",
              " (' hottest', 0.0044308784),\n",
              " (' greatest', 0.0027584091),\n",
              " (' smartest', 0.0020890823),\n",
              " (' biggest', 0.0017668337),\n",
              " (' top', 0.0014481086)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BpGshRGpLEj6",
        "outputId": "648567f5-9004-4153-a3c6-eb2f3f83b4b1"
      },
      "source": [
        "tokenizer.decode(3)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<unk>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_re7EH_gYW"
      },
      "source": [
        "# Evaluating Conditional MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfuLDpC3H-qk"
      },
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained('/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional/checkpoint-40').eval()"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X4LrdmxMVoO"
      },
      "source": [
        "## Positive sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qJw7_bCTIMLL",
        "outputId": "95c3432f-978e-4d9f-b007-6cd24d09b045"
      },
      "source": [
        "sentence = f'positive {tokenizer.sep_token} I think this restaurant is among the {tokenizer.mask_token} in the city'\n",
        "sentence"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positive </s> I think this restaurant is among the <mask> in the city'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGI79OT7MGyg"
      },
      "source": [
        "tokenizer_output = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = tokenizer_output['input_ids']\n",
        "with torch.no_grad():\n",
        "    logits = model(**tokenizer_output).logits"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai13i06AMMiS",
        "outputId": "4dc7366b-cb32-49b2-dc67-72e8c67704bf"
      },
      "source": [
        "masked_position = (input_ids == tokenizer.mask_token_id).long().argmax().item()\n",
        "probas = logits[0, masked_position, :].detach().softmax(0).numpy()\n",
        "words_probas = sorted(zip(vocab_words, probas), key=lambda t: t[1], reverse=True)\n",
        "words_probas[:10]"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' best', 0.9108875),\n",
              " (' finest', 0.04119079),\n",
              " (' better', 0.018189408),\n",
              " (' top', 0.014489585),\n",
              " (' hottest', 0.0024455837),\n",
              " (' greatest', 0.0018530154),\n",
              " (' great', 0.00081572926),\n",
              " (' stars', 0.0007540227),\n",
              " (' coolest', 0.00060782227),\n",
              " (' safest', 0.00051225425)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m4-sWS_SMQMN",
        "outputId": "c9b426bd-1511-41cc-9458-a59eeec7b4cf"
      },
      "source": [
        "sentence = f'negative {tokenizer.sep_token} I think this restaurant is among the {tokenizer.mask_token} in the city'\n",
        "sentence"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative </s> I think this restaurant is among the <mask> in the city'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyIKKPqvMUg3"
      },
      "source": [
        "tokenizer_output = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = tokenizer_output['input_ids']\n",
        "with torch.no_grad():\n",
        "    logits = model(**tokenizer_output).logits"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7V616PyMa3a",
        "outputId": "2ec9d521-cc56-4bb0-cb06-9576031e7d96"
      },
      "source": [
        "masked_position = (input_ids == tokenizer.mask_token_id).long().argmax().item()\n",
        "probas = logits[0, masked_position, :].detach().softmax(0).numpy()\n",
        "words_probas = sorted(zip(vocab_words, probas), key=lambda t: t[1], reverse=True)\n",
        "words_probas[:10]"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' best', 0.8484191),\n",
              " (' better', 0.045180257),\n",
              " (' worst', 0.040889587),\n",
              " (' finest', 0.03282445),\n",
              " (' top', 0.010033123),\n",
              " (' hottest', 0.0025719462),\n",
              " (' cheapest', 0.0021640842),\n",
              " (' greatest', 0.0018757473),\n",
              " (' smallest', 0.0011033808),\n",
              " (' busiest', 0.00088208786)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnE8IKSeOAF1"
      },
      "source": [
        "In a negative sentence, we have 'worst' as a second most probable word. It doesn't appear when conditioned on positive token. It does appear in the standard model as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_4e0HCYM7EC"
      },
      "source": [
        "\n",
        "# Training Conditonal MLM starting from a finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL7QrAuzM_C-",
        "outputId": "19a79626-7639-48b8-8158-5d88246e545b"
      },
      "source": [
        "!python3 run_conditional_language_modeling.py \\\n",
        "--output_dir '/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned' \\\n",
        "--model_name_or_path '/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_roberta/checkpoint-350' \\\n",
        "--tokenizer_name roberta-base \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--dataset_name yelp \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--gradient_accumulation_steps 4 \\\n",
        "--num_train_epochs 10 \\\n",
        "--logging_steps 1 \\\n",
        "--save_steps 10 \\\n",
        "--eval_steps 10 \\\n",
        "--evaluation_strategy steps \\\n",
        "--num_train_epochs 10 \\"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-07 21:59:16.077979: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/07/2020 21:59:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/07/2020 21:59:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec07_21-59-18_4892d22f056d', logging_first_step=False, logging_steps=1, save_steps=10, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "[INFO|configuration_utils.py:409] 2020-12-07 21:59:18,645 >> loading configuration file /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_roberta/checkpoint-350/config.json\n",
            "[INFO|configuration_utils.py:447] 2020-12-07 21:59:18,646 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"total_flos\": 21144950696866248,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:411] 2020-12-07 21:59:18,677 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:447] 2020-12-07 21:59:18,677 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1768] 2020-12-07 21:59:18,810 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1768] 2020-12-07 21:59:18,810 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1768] 2020-12-07 21:59:18,810 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|modeling_utils.py:938] 2020-12-07 21:59:18,886 >> loading weights file /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_roberta/checkpoint-350/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1056] 2020-12-07 21:59:30,012 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1065] 2020-12-07 21:59:30,012 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_roberta/checkpoint-350.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "Reusing dataset yelp_polarity (/root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n",
            "Reusing dataset yelp_polarity (/root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-fc3cc76f500726b8.arrow and /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-24ad08dc80bfc802.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-726ca3418d6d9d46.arrow and /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-4f519a9136cf60de.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-c7c2ccaaab3d1b04.arrow and /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-799019c0c125c4f8.arrow\n",
            "Example from conditional dataset: {'label': 0, 'text': \"negative </s> Let me just start by saying DO NOT BOOK THIS HOTEL... I will give you our experience is short then explain everything in full. Within the first 10 hours we were switched to 3 different rooms, treated like crap by all the employees and rooms smell either like cat piss or dirty trash!!! Okay here it goes:\\\\nSo this is everything off the top of my head... mix in extreme attitude from the staff as if they hate their jobs and are being forced to work there\\\\n1. We had to walk through the whole casino floor with 9 bags to the check in at 9:45pm\\\\n2. Check in line was 45 minutes which we understand is not their fault they were busy but at least be nice and smile \\\\n3. We go to the far tower with 9 bags to our room and our key doesnt work\\\\n4. We wait 25 minutes in a hot hallway for security to open our door you then acts like it isnt our room as if we are stupid and went to the wrong door and knocks over and over on the door\\\\n5. We have to add our other night online because they couldn't do it at the front desk check in, you can only book a room for that day at the front desks\\\\n6. call guest services to see where we go for a new key, she talks to us like we are a stupid child \\\\n7. We walk back down to the main lobby to  get new keys and to have them mark that we are staying in the same room, they said thats fine \\\\n8. We go over to talk to someone about all the problems and she tells us we cant stay in the same room because someone booked it for tomorrow, so she gives us a new room\\\\n9. We go get our bags and take them to the next room, same tower but up stairs... room smells funny and looks nothing like the pictures and has an old school tv in it\\\\n10. The light in our new bathroom is broken, we say f##k it we havent ate all day so lets go eat first\\\\n11. Its after midnight so everything is closed except a un appealing Deli, Krispy Cream and Vince Neils restaurant so we eat there \\\\n12. Back at the room we call to have the light fixed 2 hours later they still haven't come to fix it  so we cancel so we can go to bed\\\\n13. We wake up and call to have the light fixed. She hangs up on me after I tell her what's going on\\\\n14. We call to speak to a manager, they hang up on us \\\\n15. We call back and demand to speak to a manager, he finally gets on the phone we explain everything he says he is moving our room and dropping the charge for the night\\\\n16. I call bell service while my friend goes down and talks to the manager and get new keys, they tell me to call back because they arent going to come up get the bags and bring them down and then go back up (well what the heck  that's what we have been doing since we get there!!!) \\\\n17. Maid service just walks in no knock nothing and then is bitchy because we are still in the room and she is going to have to come back\\\\n18. We get the new room number, bell man comes up gets  The bags and takes us in a service elevator to our new room which is better then the last but smells like a dirty trash in a woman's restroom (if you know what I mean... dirty sanitary stuff) and again looks nothing like the pictures online\\\\n19. We have to go down stairs because it looks like he charged the upgrade card. He says it is a refund but its in the wrong column of the receipt, they're charge column is actually credits and the credits is actually charges... if this is true why don't they just fix that in the system?\\\\n20. We left a item in our room and called to tell them, they said it hadn't been turned in and to try back. In the next 24 hours before we left we called 3 times and each time they said no it hasn't been turned in or reported. So basically we know it was left on the bed and now our childs doll that we know was left in the bed magically got up and walked away within 2 hours from the first time we called!!! \\\\n\\\\nThe employees are so rude and just when you think something has been fixed for you something else goes wrong and they give you an attitude about it and talk down to you as if some how you being in the hotel is a bother. CUSTOMER SERVICE TRAINING IS NEEDED BY ALL STAFF!!! \\\\n\\\\nMind you my husband (who was not with us) works at a casino in San Diego and he was appalled by everything we were telling him, he said their hotel would be ashamed and be doing everything they could to fix everything and everything would have been comp'd (food everything) not just one night. I wasnt asking for all that I would have been happy with an apology, fixing the issue right away, no rudeness or something. I mean they treat you like they could careless if you come back and like you are bothering them by being there. Mind you we were very passive and not angry or rude at all like we should have been, and like the other people who were down at the front desks complaining!!! I don't like leaving bad reviews but this one is much needed!!!\"}\n",
            "100% 1/1 [00:00<00:00,  2.36ba/s]\n",
            "100% 1/1 [00:00<00:00, 41.81ba/s]\n",
            "100% 1/1 [00:02<00:00,  2.35s/ba]\n",
            "100% 1/1 [00:00<00:00, 77.06ba/s]\n",
            "[INFO|trainer.py:357] 2020-12-07 21:59:44,039 >> The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:357] 2020-12-07 21:59:44,040 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:662] 2020-12-07 21:59:44,045 >> ***** Running training *****\n",
            "[INFO|trainer.py:663] 2020-12-07 21:59:44,045 >>   Num examples = 332\n",
            "[INFO|trainer.py:664] 2020-12-07 21:59:44,046 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:665] 2020-12-07 21:59:44,046 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:666] 2020-12-07 21:59:44,046 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:667] 2020-12-07 21:59:44,046 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:668] 2020-12-07 21:59:44,046 >>   Total optimization steps = 100\n",
            "[INFO|trainer.py:681] 2020-12-07 21:59:44,372 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:682] 2020-12-07 21:59:44,373 >>   Continuing training from epoch 0\n",
            "[INFO|trainer.py:683] 2020-12-07 21:59:44,373 >>   Continuing training from global step 0\n",
            "[INFO|trainer.py:684] 2020-12-07 21:59:44,373 >>   Will skip the first 0 batches in the first epoch\n",
            "{'loss': 1.6379146575927734, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.09523809523809523}\n",
            "{'loss': 1.6264070272445679, 'learning_rate': 4.9e-05, 'epoch': 0.19047619047619047}\n",
            "{'loss': 1.6818324327468872, 'learning_rate': 4.85e-05, 'epoch': 0.2857142857142857}\n",
            "{'loss': 1.7258152961730957, 'learning_rate': 4.8e-05, 'epoch': 0.38095238095238093}\n",
            "{'loss': 1.685551404953003, 'learning_rate': 4.75e-05, 'epoch': 0.47619047619047616}\n",
            "{'loss': 1.779465913772583, 'learning_rate': 4.7e-05, 'epoch': 0.5714285714285714}\n",
            "{'loss': 1.608805537223816, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.6666666666666666}\n",
            "{'loss': 1.6632963418960571, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.7619047619047619}\n",
            "{'loss': 1.6245791912078857, 'learning_rate': 4.55e-05, 'epoch': 0.8571428571428571}\n",
            "{'loss': 1.6689772605895996, 'learning_rate': 4.5e-05, 'epoch': 0.9523809523809523}\n",
            " 10% 10/100 [00:42<06:22,  4.25s/it][INFO|trainer.py:1333] 2020-12-07 22:00:26,422 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:00:26,423 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:00:26,423 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  5.22it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.564754843711853, 'epoch': 0.9523809523809523}\n",
            " 10% 10/100 [00:42<06:22,  4.25s/it]\n",
            "100% 3/3 [00:00<00:00,  3.99it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:00:27,355 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-10\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:00:27,361 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-10/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:00:29,984 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-10/pytorch_model.bin\n",
            "{'loss': 2.5457468032836914, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.0952380952380953}\n",
            "{'loss': 1.6399959325790405, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.1904761904761905}\n",
            "{'loss': 1.614221453666687, 'learning_rate': 4.35e-05, 'epoch': 1.2857142857142856}\n",
            "{'loss': 1.5335049629211426, 'learning_rate': 4.3e-05, 'epoch': 1.380952380952381}\n",
            "{'loss': 1.7156049013137817, 'learning_rate': 4.25e-05, 'epoch': 1.4761904761904763}\n",
            "{'loss': 1.6636083126068115, 'learning_rate': 4.2e-05, 'epoch': 1.5714285714285714}\n",
            "{'loss': 1.6407344341278076, 'learning_rate': 4.15e-05, 'epoch': 1.6666666666666665}\n",
            "{'loss': 1.6585643291473389, 'learning_rate': 4.1e-05, 'epoch': 1.7619047619047619}\n",
            "{'loss': 1.7892876863479614, 'learning_rate': 4.05e-05, 'epoch': 1.8571428571428572}\n",
            "{'loss': 1.6105713844299316, 'learning_rate': 4e-05, 'epoch': 1.9523809523809523}\n",
            " 20% 20/100 [01:37<06:12,  4.65s/it][INFO|trainer.py:1333] 2020-12-07 22:01:22,221 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:01:22,221 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:01:22,221 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.84it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.584890365600586, 'epoch': 1.9523809523809523}\n",
            " 20% 20/100 [01:38<06:12,  4.65s/it]\n",
            "100% 3/3 [00:00<00:00,  3.75it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:01:23,207 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-20\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:01:23,218 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-20/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:01:25,861 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-20/pytorch_model.bin\n",
            "{'loss': 2.4443225860595703, 'learning_rate': 3.9500000000000005e-05, 'epoch': 2.0952380952380953}\n",
            "{'loss': 1.6013779640197754, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.1904761904761907}\n",
            "{'loss': 1.4699914455413818, 'learning_rate': 3.85e-05, 'epoch': 2.2857142857142856}\n",
            "{'loss': 1.6265318393707275, 'learning_rate': 3.8e-05, 'epoch': 2.380952380952381}\n",
            "{'loss': 1.6442592144012451, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.4761904761904763}\n",
            "{'loss': 1.6638360023498535, 'learning_rate': 3.7e-05, 'epoch': 2.571428571428571}\n",
            "{'loss': 1.533189058303833, 'learning_rate': 3.65e-05, 'epoch': 2.6666666666666665}\n",
            "{'loss': 1.6353191137313843, 'learning_rate': 3.6e-05, 'epoch': 2.761904761904762}\n",
            "{'loss': 1.5494074821472168, 'learning_rate': 3.55e-05, 'epoch': 2.857142857142857}\n",
            "{'loss': 1.5206044912338257, 'learning_rate': 3.5e-05, 'epoch': 2.9523809523809526}\n",
            " 30% 30/100 [02:35<05:36,  4.81s/it][INFO|trainer.py:1333] 2020-12-07 22:02:20,057 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:02:20,057 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:02:20,057 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.70it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.5331519842147827, 'epoch': 2.9523809523809526}\n",
            " 30% 30/100 [02:36<05:36,  4.81s/it]\n",
            "100% 3/3 [00:01<00:00,  3.63it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:02:21,082 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-30\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:02:21,090 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-30/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:02:23,932 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-30/pytorch_model.bin\n",
            "{'loss': 2.4095191955566406, 'learning_rate': 3.45e-05, 'epoch': 3.0952380952380953}\n",
            "{'loss': 1.6119065284729004, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.1904761904761907}\n",
            "{'loss': 1.536454677581787, 'learning_rate': 3.35e-05, 'epoch': 3.2857142857142856}\n",
            "{'loss': 1.6585620641708374, 'learning_rate': 3.3e-05, 'epoch': 3.380952380952381}\n",
            "{'loss': 1.6027913093566895, 'learning_rate': 3.2500000000000004e-05, 'epoch': 3.4761904761904763}\n",
            "{'loss': 1.441107153892517, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.571428571428571}\n",
            "{'loss': 1.5182909965515137, 'learning_rate': 3.15e-05, 'epoch': 3.6666666666666665}\n",
            "{'loss': 1.4952468872070312, 'learning_rate': 3.1e-05, 'epoch': 3.761904761904762}\n",
            "{'loss': 1.5691008567810059, 'learning_rate': 3.05e-05, 'epoch': 3.857142857142857}\n",
            "{'loss': 1.6479833126068115, 'learning_rate': 3e-05, 'epoch': 3.9523809523809526}\n",
            " 40% 40/100 [03:34<04:57,  4.95s/it][INFO|trainer.py:1333] 2020-12-07 22:03:18,990 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:03:18,990 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:03:18,990 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.60it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.3735262155532837, 'epoch': 3.9523809523809526}\n",
            " 40% 40/100 [03:35<04:57,  4.95s/it]\n",
            "100% 3/3 [00:01<00:00,  3.49it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:03:20,054 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-40\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:03:20,061 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-40/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:03:22,933 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-40/pytorch_model.bin\n",
            "{'loss': 2.4882516860961914, 'learning_rate': 2.95e-05, 'epoch': 4.095238095238095}\n",
            "{'loss': 1.516229510307312, 'learning_rate': 2.9e-05, 'epoch': 4.190476190476191}\n",
            "{'loss': 1.5880205631256104, 'learning_rate': 2.8499999999999998e-05, 'epoch': 4.285714285714286}\n",
            "{'loss': 1.5706839561462402, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.380952380952381}\n",
            "{'loss': 1.522223949432373, 'learning_rate': 2.7500000000000004e-05, 'epoch': 4.476190476190476}\n",
            "{'loss': 1.5928899049758911, 'learning_rate': 2.7000000000000002e-05, 'epoch': 4.571428571428571}\n",
            "{'loss': 1.5539581775665283, 'learning_rate': 2.6500000000000004e-05, 'epoch': 4.666666666666667}\n",
            "{'loss': 1.5132883787155151, 'learning_rate': 2.6000000000000002e-05, 'epoch': 4.761904761904762}\n",
            "{'loss': 1.571816086769104, 'learning_rate': 2.5500000000000003e-05, 'epoch': 4.857142857142857}\n",
            "{'loss': 1.5589903593063354, 'learning_rate': 2.5e-05, 'epoch': 4.9523809523809526}\n",
            " 50% 50/100 [04:35<04:14,  5.09s/it][INFO|trainer.py:1333] 2020-12-07 22:04:19,410 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:04:19,410 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:04:19,410 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.46it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.4750216007232666, 'epoch': 4.9523809523809526}\n",
            " 50% 50/100 [04:36<04:14,  5.09s/it]\n",
            "100% 3/3 [00:01<00:00,  3.36it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:04:20,516 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-50\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:04:20,524 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-50/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:04:23,590 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-50/pytorch_model.bin\n",
            "{'loss': 2.265160083770752, 'learning_rate': 2.45e-05, 'epoch': 5.095238095238095}\n",
            "{'loss': 1.4557223320007324, 'learning_rate': 2.4e-05, 'epoch': 5.190476190476191}\n",
            "{'loss': 1.4980822801589966, 'learning_rate': 2.35e-05, 'epoch': 5.285714285714286}\n",
            "{'loss': 1.5072627067565918, 'learning_rate': 2.3000000000000003e-05, 'epoch': 5.380952380952381}\n",
            "{'loss': 1.5941022634506226, 'learning_rate': 2.25e-05, 'epoch': 5.476190476190476}\n",
            "{'loss': 1.5486574172973633, 'learning_rate': 2.2000000000000003e-05, 'epoch': 5.571428571428571}\n",
            "{'loss': 1.6044857501983643, 'learning_rate': 2.15e-05, 'epoch': 5.666666666666667}\n",
            "{'loss': 1.446822166442871, 'learning_rate': 2.1e-05, 'epoch': 5.761904761904762}\n",
            "{'loss': 1.5096344947814941, 'learning_rate': 2.05e-05, 'epoch': 5.857142857142857}\n",
            "{'loss': 1.5208758115768433, 'learning_rate': 2e-05, 'epoch': 5.9523809523809526}\n",
            " 60% 60/100 [05:36<03:25,  5.13s/it][INFO|trainer.py:1333] 2020-12-07 22:05:20,926 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:05:20,926 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:05:20,926 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.46it/s]\u001b[A\n",
            "100% 3/3 [00:00<00:00,  3.32it/s]\u001b[A\n",
            "{'eval_loss': 1.500138282775879, 'epoch': 5.9523809523809526}\n",
            "\n",
            " 60% 60/100 [05:37<03:25,  5.13s/it]\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:05:22,034 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-60\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:05:22,042 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-60/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:05:24,904 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-60/pytorch_model.bin\n",
            "{'loss': 2.2200803756713867, 'learning_rate': 1.9500000000000003e-05, 'epoch': 6.095238095238095}\n",
            "{'loss': 1.6184436082839966, 'learning_rate': 1.9e-05, 'epoch': 6.190476190476191}\n",
            "{'loss': 1.5641140937805176, 'learning_rate': 1.85e-05, 'epoch': 6.285714285714286}\n",
            "{'loss': 1.5100882053375244, 'learning_rate': 1.8e-05, 'epoch': 6.380952380952381}\n",
            "{'loss': 1.6046537160873413, 'learning_rate': 1.75e-05, 'epoch': 6.476190476190476}\n",
            "{'loss': 1.4500072002410889, 'learning_rate': 1.7000000000000003e-05, 'epoch': 6.571428571428571}\n",
            "{'loss': 1.447551965713501, 'learning_rate': 1.65e-05, 'epoch': 6.666666666666667}\n",
            "{'loss': 1.5681211948394775, 'learning_rate': 1.6000000000000003e-05, 'epoch': 6.761904761904762}\n",
            "{'loss': 1.4303948879241943, 'learning_rate': 1.55e-05, 'epoch': 6.857142857142857}\n",
            "{'loss': 1.4809513092041016, 'learning_rate': 1.5e-05, 'epoch': 6.9523809523809526}\n",
            " 70% 70/100 [06:38<02:36,  5.20s/it][INFO|trainer.py:1333] 2020-12-07 22:06:22,769 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:06:22,770 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:06:22,770 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.40it/s]\u001b[A\n",
            "100% 3/3 [00:00<00:00,  3.28it/s]\u001b[A\n",
            "{'eval_loss': 1.459717035293579, 'epoch': 6.9523809523809526}\n",
            "\n",
            " 70% 70/100 [06:39<02:36,  5.20s/it]\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:06:23,892 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-70\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:06:23,899 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-70/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:06:26,787 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-70/pytorch_model.bin\n",
            "{'loss': 2.166700839996338, 'learning_rate': 1.45e-05, 'epoch': 7.095238095238095}\n",
            "{'loss': 1.451507568359375, 'learning_rate': 1.4000000000000001e-05, 'epoch': 7.190476190476191}\n",
            "{'loss': 1.539780616760254, 'learning_rate': 1.3500000000000001e-05, 'epoch': 7.285714285714286}\n",
            "{'loss': 1.5141358375549316, 'learning_rate': 1.3000000000000001e-05, 'epoch': 7.380952380952381}\n",
            "{'loss': 1.5503019094467163, 'learning_rate': 1.25e-05, 'epoch': 7.476190476190476}\n",
            "{'loss': 1.535283088684082, 'learning_rate': 1.2e-05, 'epoch': 7.571428571428571}\n",
            "{'loss': 1.4100492000579834, 'learning_rate': 1.1500000000000002e-05, 'epoch': 7.666666666666667}\n",
            "{'loss': 1.509554147720337, 'learning_rate': 1.1000000000000001e-05, 'epoch': 7.761904761904762}\n",
            "{'loss': 1.5291285514831543, 'learning_rate': 1.05e-05, 'epoch': 7.857142857142857}\n",
            "{'loss': 1.6704299449920654, 'learning_rate': 1e-05, 'epoch': 7.9523809523809526}\n",
            " 80% 80/100 [07:41<01:45,  5.26s/it][INFO|trainer.py:1333] 2020-12-07 22:07:25,433 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:07:25,433 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:07:25,433 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.37it/s]\u001b[A\n",
            "100% 3/3 [00:00<00:00,  3.27it/s]\u001b[A\n",
            "{'eval_loss': 1.5555652379989624, 'epoch': 7.9523809523809526}\n",
            "\n",
            " 80% 80/100 [07:42<01:45,  5.26s/it]\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:07:26,561 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-80\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:07:26,572 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-80/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:07:29,750 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-80/pytorch_model.bin\n",
            "{'loss': 2.2119641304016113, 'learning_rate': 9.5e-06, 'epoch': 8.095238095238095}\n",
            "{'loss': 1.5089069604873657, 'learning_rate': 9e-06, 'epoch': 8.19047619047619}\n",
            "{'loss': 1.6125645637512207, 'learning_rate': 8.500000000000002e-06, 'epoch': 8.285714285714286}\n",
            "{'loss': 1.441176414489746, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.380952380952381}\n",
            "{'loss': 1.4977675676345825, 'learning_rate': 7.5e-06, 'epoch': 8.476190476190476}\n",
            "{'loss': 1.4741201400756836, 'learning_rate': 7.000000000000001e-06, 'epoch': 8.571428571428571}\n",
            "{'loss': 1.5167051553726196, 'learning_rate': 6.5000000000000004e-06, 'epoch': 8.666666666666666}\n",
            "{'loss': 1.4393329620361328, 'learning_rate': 6e-06, 'epoch': 8.761904761904763}\n",
            "{'loss': 1.4941153526306152, 'learning_rate': 5.500000000000001e-06, 'epoch': 8.857142857142858}\n",
            "{'loss': 1.3985127210617065, 'learning_rate': 5e-06, 'epoch': 8.952380952380953}\n",
            " 90% 90/100 [08:44<00:53,  5.30s/it][INFO|trainer.py:1333] 2020-12-07 22:08:28,936 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:08:28,936 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:08:28,936 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.44it/s]\u001b[A\n",
            "100% 3/3 [00:00<00:00,  3.26it/s]\u001b[A\n",
            "{'eval_loss': 1.4547014236450195, 'epoch': 8.952380952380953}\n",
            "\n",
            " 90% 90/100 [08:45<00:53,  5.30s/it]\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:08:30,061 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-90\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:08:30,073 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-90/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:08:32,884 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-90/pytorch_model.bin\n",
            "{'loss': 2.2172093391418457, 'learning_rate': 4.5e-06, 'epoch': 9.095238095238095}\n",
            "{'loss': 1.497819423675537, 'learning_rate': 4.000000000000001e-06, 'epoch': 9.19047619047619}\n",
            "{'loss': 1.3743888139724731, 'learning_rate': 3.5000000000000004e-06, 'epoch': 9.285714285714286}\n",
            "{'loss': 1.5490972995758057, 'learning_rate': 3e-06, 'epoch': 9.380952380952381}\n",
            "{'loss': 1.5481771230697632, 'learning_rate': 2.5e-06, 'epoch': 9.476190476190476}\n",
            "{'loss': 1.4247150421142578, 'learning_rate': 2.0000000000000003e-06, 'epoch': 9.571428571428571}\n",
            "{'loss': 1.4804763793945312, 'learning_rate': 1.5e-06, 'epoch': 9.666666666666666}\n",
            "{'loss': 1.524595856666565, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.761904761904763}\n",
            "{'loss': 1.431598424911499, 'learning_rate': 5.000000000000001e-07, 'epoch': 9.857142857142858}\n",
            "{'loss': 1.5080835819244385, 'learning_rate': 0.0, 'epoch': 9.952380952380953}\n",
            "100% 100/100 [09:47<00:00,  5.33s/it][INFO|trainer.py:1333] 2020-12-07 22:09:31,960 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:09:31,960 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:09:31,960 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  4.41it/s]\u001b[A\n",
            "100% 3/3 [00:00<00:00,  3.26it/s]\u001b[A\n",
            "{'eval_loss': 1.5426677465438843, 'epoch': 9.952380952380953}\n",
            "\n",
            "100% 100/100 [09:48<00:00,  5.33s/it]\n",
            "                                 \u001b[A[INFO|trainer.py:1162] 2020-12-07 22:09:33,091 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-100\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:09:33,098 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-100/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:09:35,782 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-100/pytorch_model.bin\n",
            "[INFO|trainer.py:801] 2020-12-07 22:09:44,473 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 9.952380952380953}\n",
            "100% 100/100 [10:00<00:00,  6.00s/it]\n",
            "[INFO|trainer.py:1162] 2020-12-07 22:09:44,477 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned\n",
            "[INFO|configuration_utils.py:281] 2020-12-07 22:09:44,483 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/config.json\n",
            "[INFO|modeling_utils.py:741] 2020-12-07 22:09:55,219 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/pytorch_model.bin\n",
            "12/07/2020 22:09:55 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1333] 2020-12-07 22:09:55,543 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1334] 2020-12-07 22:09:55,544 >>   Num examples = 19\n",
            "[INFO|trainer.py:1335] 2020-12-07 22:09:55,544 >>   Batch size = 8\n",
            "100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "12/07/2020 22:09:57 - INFO - __main__ -   ***** Eval results *****\n",
            "12/07/2020 22:09:57 - INFO - __main__ -     perplexity = 4.3658027990595105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPFW73MINK7Q"
      },
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained('/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-40').eval()"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFbToYSeQoky",
        "outputId": "5ec496bb-3842-4dde-b2ae-74c06c59a485"
      },
      "source": [
        "sentence = f'positive {tokenizer.sep_token} I think this restaurant is among the {tokenizer.mask_token} in the city'\n",
        "\n",
        "tokenizer_output = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = tokenizer_output['input_ids']\n",
        "with torch.no_grad():\n",
        "    logits = model(**tokenizer_output).logits\n",
        "\n",
        "masked_position = (input_ids == tokenizer.mask_token_id).long().argmax().item()\n",
        "probas = logits[0, masked_position, :].detach().softmax(0).numpy()\n",
        "words_probas = sorted(zip(vocab_words, probas), key=lambda t: t[1], reverse=True)\n",
        "words_probas[:10]"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' best', 0.92760354),\n",
              " (' better', 0.028655292),\n",
              " (' finest', 0.019014917),\n",
              " (' top', 0.010744136),\n",
              " (' greatest', 0.0019361579),\n",
              " (' hottest', 0.0011975307),\n",
              " (' worst', 0.00088472496),\n",
              " (' great', 0.00066560134),\n",
              " (' BEST', 0.0005510724),\n",
              " (' stars', 0.00053330633)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da_jchylQv9j",
        "outputId": "20db75f6-463d-4e6a-8137-0f521a268d64"
      },
      "source": [
        "sentence = f'negative {tokenizer.sep_token} I think this restaurant is among the {tokenizer.mask_token} in the city'\n",
        "\n",
        "tokenizer_output = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = tokenizer_output['input_ids']\n",
        "with torch.no_grad():\n",
        "    logits = model(**tokenizer_output).logits\n",
        "\n",
        "masked_position = (input_ids == tokenizer.mask_token_id).long().argmax().item()\n",
        "probas = logits[0, masked_position, :].detach().softmax(0).numpy()\n",
        "words_probas = sorted(zip(vocab_words, probas), key=lambda t: t[1], reverse=True)\n",
        "words_probas[:10]"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' best', 0.4735862),\n",
              " (' worst', 0.43094367),\n",
              " (' better', 0.044027366),\n",
              " (' worse', 0.009041737),\n",
              " (' finest', 0.007861966),\n",
              " (' cheapest', 0.00685714),\n",
              " (' top', 0.0043341867),\n",
              " (' smallest', 0.0027911544),\n",
              " (' weakest', 0.0020172282),\n",
              " (' poorest', 0.0015362091)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyPkTa6PQ4iD"
      },
      "source": [
        "This works much better. 'Worst' is still second most probable, but this time it has probability of 43%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbEshb-eSII3"
      },
      "source": [
        "## Last checkpoint\n",
        "We took fourth checkpoint because it had lower validation loss. However, validation loss is noisy (validation dataset has only 50 examples). Let's take the latest checkpoint to see how it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biTZf_GBSNKl"
      },
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained('/content/drive/MyDrive/Colab Notebooks/nlp/pretrained_models/yelp_conditional_finetuned/checkpoint-100').eval()"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaM-Mu5vSP4_",
        "outputId": "432a74ee-32df-440c-95b2-2d15dddf7508"
      },
      "source": [
        "sentence = f'positive {tokenizer.sep_token} I think this restaurant is among the {tokenizer.mask_token} in the city'\n",
        "\n",
        "tokenizer_output = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = tokenizer_output['input_ids']\n",
        "with torch.no_grad():\n",
        "    logits = model(**tokenizer_output).logits\n",
        "\n",
        "masked_position = (input_ids == tokenizer.mask_token_id).long().argmax().item()\n",
        "probas = logits[0, masked_position, :].detach().softmax(0).numpy()\n",
        "words_probas = sorted(zip(vocab_words, probas), key=lambda t: t[1], reverse=True)\n",
        "words_probas[:10]"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' best', 0.9244944),\n",
              " (' better', 0.032192133),\n",
              " (' finest', 0.021731451),\n",
              " (' top', 0.006033973),\n",
              " (' greatest', 0.0022383414),\n",
              " (' worst', 0.0013920541),\n",
              " (' safest', 0.0010354719),\n",
              " (' great', 0.0008828963),\n",
              " (' hottest', 0.0008741495),\n",
              " (' cheapest', 0.00076614734)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coeSqQ6sSSIZ",
        "outputId": "eeed8490-5c2e-42a8-ecd7-83e702c99992"
      },
      "source": [
        "sentence = f'negative {tokenizer.sep_token} I think this restaurant is among the {tokenizer.mask_token} in the city'\n",
        "\n",
        "tokenizer_output = tokenizer(sentence, return_tensors='pt')\n",
        "input_ids = tokenizer_output['input_ids']\n",
        "with torch.no_grad():\n",
        "    logits = model(**tokenizer_output).logits\n",
        "\n",
        "masked_position = (input_ids == tokenizer.mask_token_id).long().argmax().item()\n",
        "probas = logits[0, masked_position, :].detach().softmax(0).numpy()\n",
        "words_probas = sorted(zip(vocab_words, probas), key=lambda t: t[1], reverse=True)\n",
        "words_probas[:10]"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' worst', 0.59666413),\n",
              " (' best', 0.31662795),\n",
              " (' better', 0.03332686),\n",
              " (' worse', 0.01389257),\n",
              " (' cheapest', 0.010395007),\n",
              " (' finest', 0.006077247),\n",
              " (' smallest', 0.0027908573),\n",
              " (' weakest', 0.001942544),\n",
              " (' top', 0.0016419988),\n",
              " (' poorest', 0.0015085399)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKXKMGeTCKN"
      },
      "source": [
        "It does even better after more training. 'Worst' is the most probable word here, this model seems to capture sentiment quite well. Maybe it would keep improving after more training - let's check it."
      ]
    }
  ]
}